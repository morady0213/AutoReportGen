{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "935828dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss # Using CPU version now\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "import pandas as pd\n",
    "import base64\n",
    "import io\n",
    "import json # For embedding JS data and parsing LLM output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15569e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ollama Client Library\n",
    "try:\n",
    "    import ollama\n",
    "    OLLAMA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"WARNING: Ollama library not found. Triple extraction/generation skipped. Install with: pip install ollama\")\n",
    "    ollama = None\n",
    "    OLLAMA_AVAILABLE = False\n",
    "\n",
    "# Hugging Face Libraries\n",
    "from transformers import (\n",
    "    AutoProcessor, AutoModel, AutoTokenizer,\n",
    "    pipeline, BitsAndBytesConfig\n",
    ")\n",
    "# Sentence Transformers no longer needed for fine-tuning\n",
    "\n",
    "# Scikit-learn for cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Configuration\n",
    "# ------------------------------------------\n",
    "CONFIG = {\n",
    "    \"scan_dir\": r\"D:\\NLP apps\\Scans\",\n",
    "    \"report_dir\": r\"D:\\NLP apps\\Reports\",\n",
    "    \"num_reports_to_process\": 150,  # Keep low for testing KG vis\n",
    "    \"max_reports_total\": 3999,\n",
    "    \"output_dir\": r\"D:\\NLP apps\\radiology_rag_kg_vis_output_v3\", # New output dir\n",
    "\n",
    "    # --- Triple Extraction (using Ollama Llama3) ---\n",
    "    \"triple_extractor_model\": \"llama3\", # Ollama model for extraction\n",
    "    \"ollama_base_url\": \"http://localhost:11434\", # Default Ollama API endpoint\n",
    "\n",
    "    # --- Embedding Model (CLIP for Retrieval & Evaluation) ---\n",
    "    \"embedding_model_name\": \"openai/clip-vit-base-patch32\", # Use CLIP\n",
    "\n",
    "    # --- RAG Components ---\n",
    "    # Generator Model (Using Ollama)\n",
    "    \"generator_type\": \"ollama\",\n",
    "    \"ollama_generator_model\": \"llava-llama3\", # Separate model for generation\n",
    "    \"ollama_num_ctx\": 4096,\n",
    "    # Retrieval\n",
    "    \"top_k_retrieval\": 3,\n",
    "\n",
    "    # --- Evaluation Metrics ---\n",
    "    \"eval_embedding_weight\": 0.7, # Weight for embedding similarity in combined score\n",
    "    \"eval_graph_similarity_weight\": 0.3, # Weight for graph similarity\n",
    "\n",
    "    # --- Hardware ---\n",
    "    \"use_gpu\": torch.cuda.is_available(),\n",
    "    \"embedding_device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"faiss_use_gpu\": False, # Sticking to CPU Faiss\n",
    "    \"generator_device\": \"cpu\", # Ollama runs externally\n",
    "    \"triple_extractor_device\": \"cpu\", # Ollama runs externally\n",
    "}\n",
    "# ------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca4e2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Helper Functions ---\n",
    "def cleanup_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "def extract_text_from_xml(xml_path):\n",
    "    try:\n",
    "        tree = ET.parse(xml_path); root = tree.getroot(); texts = []\n",
    "        for tag in ['AbstractText', 'FINDINGS', 'IMPRESSION', 'REPORT_TEXT', 'paragraph']:\n",
    "             for elem in root.findall(f'.//{tag}'):\n",
    "                 if elem.text:\n",
    "                     cleaned_text = re.sub(r'\\s+', ' ', elem.text.strip())\n",
    "                     if cleaned_text: texts.append(cleaned_text)\n",
    "        if not texts:\n",
    "             all_text = ' '.join(node.text.strip() for node in root.iter() if node.text and node.text.strip())\n",
    "             if all_text: all_text = re.sub(r'\\s+', ' ', all_text).strip(); texts.append(all_text)\n",
    "        full_text = \"\\n\".join(texts); return re.sub(r'\\s+', ' ', full_text).strip()\n",
    "    except Exception: return None\n",
    "\n",
    "def find_images_for_report(report_id, scan_dir):\n",
    "    front_pattern = os.path.join(scan_dir, f\"CXR{report_id}_*_IM-*-4*.[Pp][Nn][Gg]\")\n",
    "    side_pattern = os.path.join(scan_dir, f\"CXR{report_id}_*_IM-*-3*.[Pp][Nn][Gg]\")\n",
    "    front_images = glob.glob(front_pattern); side_images = glob.glob(side_pattern)\n",
    "    front_image_path = front_images[0] if front_images else None\n",
    "    side_image_path = side_images[0] if side_images else None\n",
    "    if not front_image_path and not side_image_path:\n",
    "         any_pattern = os.path.join(scan_dir, f\"CXR{report_id}_*.[Pp][Nn][Gg]\")\n",
    "         any_images = sorted(glob.glob(any_pattern))\n",
    "         if len(any_images) >= 1: front_image_path = any_images[0]\n",
    "         if len(any_images) >= 2: side_image_path = any_images[1]\n",
    "         if not front_image_path and not side_image_path:\n",
    "            jpg_pattern = os.path.join(scan_dir, f\"CXR{report_id}_*.[Jj][Pp][Gg]\")\n",
    "            jpeg_pattern = os.path.join(scan_dir, f\"CXR{report_id}_*.[Jj][Pp][Ee][Gg]\")\n",
    "            jpg_images = sorted(glob.glob(jpg_pattern) + glob.glob(jpeg_pattern))\n",
    "            if len(jpg_images) >= 1: front_image_path = jpg_images[0]\n",
    "            if len(jpg_images) >= 2: side_image_path = jpg_images[1]\n",
    "    return front_image_path, side_image_path\n",
    "\n",
    "def encode_image_to_base64(image_path):\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            if img.mode != 'RGB': img = img.convert('RGB')\n",
    "            buffer = io.BytesIO(); img.save(buffer, format=\"JPEG\")\n",
    "            img_bytes = buffer.getvalue(); base64_string = base64.b64encode(img_bytes).decode('utf-8')\n",
    "            return base64_string\n",
    "    except Exception as e: print(f\"Error encoding image {image_path}: {e}\"); return None\n",
    "\n",
    "def calculate_graph_similarity(triples1, triples2):\n",
    "    \"\"\" Calculates simple Jaccard similarity based on entities and predicates. \"\"\"\n",
    "    if not triples1 or not triples2: return 0.0\n",
    "    entities1 = set(s for s,p,o in triples1) | set(o for s,p,o in triples1)\n",
    "    entities2 = set(s for s,p,o in triples2) | set(o for s,p,o in triples2)\n",
    "    predicates1 = set(p for s,p,o in triples1)\n",
    "    predicates2 = set(p for s,p,o in triples2)\n",
    "    entity_intersect = len(entities1.intersection(entities2)); entity_union = len(entities1.union(entities2))\n",
    "    entity_sim = entity_intersect / entity_union if entity_union > 0 else 0\n",
    "    predicate_intersect = len(predicates1.intersection(predicates2)); predicate_union = len(predicates1.union(predicates2))\n",
    "    predicate_sim = predicate_intersect / predicate_union if predicate_union > 0 else 0\n",
    "    # Give slightly more weight to entity overlap? Or keep simple average.\n",
    "    return (entity_sim + predicate_sim) / 2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa1f230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Core Classes ---\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Loads reports and associated images.\"\"\"\n",
    "    def __init__(self, report_dir, scan_dir, num_to_load, max_total):\n",
    "        self.report_dir = report_dir; self.scan_dir = scan_dir\n",
    "        self.num_to_load = min(num_to_load, max_total); self.max_total = max_total\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Loads report texts, IDs, and image paths.\"\"\"\n",
    "        data = []; report_files = sorted(glob.glob(os.path.join(self.report_dir, \"*.[Xx][Mm][Ll]\")))\n",
    "        if not report_files: raise FileNotFoundError(f\"No XML reports found in {self.report_dir}\")\n",
    "        print(f\"Found {len(report_files)} reports. Processing up to {self.num_to_load}...\")\n",
    "        processed_count, skipped_count = 0, 0\n",
    "        for report_path in tqdm(report_files, desc=\"Loading Reports\"):\n",
    "            if processed_count >= self.num_to_load: break\n",
    "            report_filename = os.path.basename(report_path)\n",
    "            report_id_match = re.match(r\"(\\d+)\\.[Xx][Mm][Ll]\", report_filename, re.IGNORECASE)\n",
    "            if not report_id_match: continue\n",
    "            report_id = report_id_match.group(1); report_text = extract_text_from_xml(report_path)\n",
    "            if report_text:\n",
    "                front_img, side_img = find_images_for_report(report_id, self.scan_dir)\n",
    "                if front_img: # Require at least front image\n",
    "                    data.append({\"report_id\": report_id, \"report_path\": report_path, \"report_text\": report_text,\n",
    "                                 \"front_image_path\": front_img, \"side_image_path\": side_img})\n",
    "                    processed_count += 1\n",
    "                else: skipped_count += 1\n",
    "            else: skipped_count += 1\n",
    "        print(f\"Successfully loaded {len(data)} reports with front images. Skipped {skipped_count}.\")\n",
    "        if len(data) < self.num_to_load: print(f\"Warning: Loaded fewer reports ({len(data)}) than requested.\")\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96294bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TripleExtractor:\n",
    "    \"\"\" Extracts triples using an Ollama model. \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.model_name = config[\"triple_extractor_model\"]\n",
    "        self.base_url = config.get(\"ollama_base_url\", \"http://localhost:11434\")\n",
    "        self.client = None\n",
    "        self._initialize_client()\n",
    "\n",
    "    def _initialize_client(self):\n",
    "        \"\"\"Initializes Ollama client.\"\"\"\n",
    "        if not OLLAMA_AVAILABLE: print(\"Ollama library not available for Triple Extractor.\"); return\n",
    "        try:\n",
    "            print(f\"Initializing Ollama client for Triple Extraction (Model: '{self.model_name}') at {self.base_url}...\")\n",
    "            self.client = ollama.Client(host=self.base_url); self.client.list()\n",
    "            print(\"Ollama client for Triple Extraction initialized.\")\n",
    "            available_models = [m['name'] for m in self.client.list()['models']]\n",
    "            if not any(m.startswith(self.model_name) for m in available_models):\n",
    "                 print(f\"Warning: Triple extractor model '{self.model_name}' not found in Ollama. Run `ollama pull {self.model_name}`.\")\n",
    "        except Exception as e: print(f\"Error initializing Ollama client for triples: {e}\"); self.client = None\n",
    "\n",
    "    def _create_extraction_prompt(self, text):\n",
    "        \"\"\" Creates the prompt for asking Llama3 to extract triples. \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "Analyze the following radiology report text. Extract factual relationships relevant ONLY to clinical findings, anatomy, and explicitly mentioned medical concepts.\n",
    "Present the relationships as a JSON list of lists, where each inner list is a triple: [Subject, Predicate, Object].\n",
    "\n",
    "Rules:\n",
    "- Subjects and Objects MUST be specific clinical entities found in the text (e.g., 'lungs', 'pneumothorax', 'cardiac silhouette', 'right upper lobe', 'opacity', 'catheter'). Normalize terms (e.g., lowercase).\n",
    "- Predicates SHOULD reflect the action or state described in the text, using verbs or short descriptive phrases where possible (e.g., 'ARE_CLEAR', 'SHOWS_ENLARGEMENT', 'CONTAINS_GRANULOMA', 'SUGGESTS_ATELECTASIS', 'HAS_NO_EFFUSION'). Use uppercase snake_case. Prefer predicates derived from the text's verbs.\n",
    "- Extract ONLY relationships explicitly stated or strongly implied in the text. Do not infer relationships not present.\n",
    "- Focus ONLY on medical facts relevant to the patient's condition as described. Ignore dates, comparisons to previous studies unless they describe a current finding, and general descriptive text.\n",
    "- If a finding is explicitly negated (e.g., \"no pneumothorax\"), use a predicate reflecting negation (e.g., ['chest', 'HAS_NO_PNEUMOTHORAX', 'pneumothorax'] or ['pneumothorax', 'IS_ABSENT', '']).\n",
    "- Output ONLY the JSON list of lists, nothing else. If no relevant triples are found, output an empty list [].\n",
    "\n",
    "Radiology Report Text:\n",
    "\\\"\\\"\\\"\n",
    "{text}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "JSON Output:\n",
    "\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def extract_triples(self, text):\n",
    "        \"\"\" Extracts triples using the Ollama model. \"\"\"\n",
    "        if not text or self.client is None: return []\n",
    "        prompt = self._create_extraction_prompt(text); triples = []\n",
    "        try:\n",
    "            response = self.client.generate(model=self.model_name, prompt=prompt, stream=False,\n",
    "                                            options={'temperature': 0.1, 'num_ctx': CONFIG.get('ollama_num_ctx', 2048)})\n",
    "            raw_output = response.get('response', '').strip()\n",
    "            try:\n",
    "                json_start = raw_output.find('['); json_end = raw_output.rfind(']') + 1\n",
    "                if json_start != -1 and json_end > json_start:\n",
    "                    json_str = raw_output[json_start:json_end]\n",
    "                    parsed_output = json.loads(json_str)\n",
    "                    if isinstance(parsed_output, list):\n",
    "                        for item in parsed_output:\n",
    "                            if isinstance(item, list) and len(item) == 3:\n",
    "                                subj = str(item[0]).lower().strip(); pred = str(item[1]).upper().strip().replace(\" \", \"_\"); obj = str(item[2]).lower().strip()\n",
    "                                if subj and pred and obj and pred.isupper() and '_' in pred:\n",
    "                                     triples.append((subj, pred, obj))\n",
    "            except json.JSONDecodeError: print(f\"Warning: Failed to parse JSON from LLM output: {raw_output}\")\n",
    "            except Exception as parse_e: print(f\"Error parsing triples from LLM output: {parse_e}\\nOutput: {raw_output}\")\n",
    "        except Exception as e: print(f\"Error during Ollama call for triple extraction: {e}\")\n",
    "        return triples\n",
    "\n",
    "    def unload_pipeline(self): pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8ace1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmbeddingManager:\n",
    "    \"\"\"Handles CLIP embeddings for retrieval and evaluation.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config; self.device = config.get(\"embedding_device\", \"cpu\")\n",
    "        print(f\"EmbeddingManager (CLIP) using device: {self.device}\")\n",
    "        self.model = None; self.processor = None; self.tokenizer = None\n",
    "        self.faiss_index = None; self.report_id_map = []; self.loaded_model_path = None\n",
    "\n",
    "    def _load_clip_model(self):\n",
    "        \"\"\"Loads CLIP model, processor, tokenizer.\"\"\"\n",
    "        model_name_or_path = self.config['embedding_model_name']\n",
    "        if self.model is None or self.loaded_model_path != model_name_or_path:\n",
    "            print(f\"Loading CLIP model/processor/tokenizer: {model_name_or_path}...\")\n",
    "            try:\n",
    "                self.processor = AutoProcessor.from_pretrained(model_name_or_path)\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "                self.model = AutoModel.from_pretrained(model_name_or_path).to(self.device)\n",
    "                self.model.eval(); self.loaded_model_path = model_name_or_path\n",
    "                print(\"CLIP components loaded.\")\n",
    "            except Exception as e: print(f\"Error loading CLIP {model_name_or_path}: {e}\"); self.model=None; self.processor=None; self.tokenizer=None; self.loaded_model_path=None; raise\n",
    "\n",
    "    def create_report_text_embeddings(self, reports_data):\n",
    "        \"\"\"Generates CLIP embeddings for full report texts (for RAG index).\"\"\"\n",
    "        self._load_clip_model()\n",
    "        if self.model is None or self.tokenizer is None: raise RuntimeError(\"CLIP model/tokenizer failed.\")\n",
    "        print(f\"Generating CLIP report text embeddings using: {self.loaded_model_path}\")\n",
    "        report_texts = [item[\"report_text\"] for item in reports_data]\n",
    "        self.report_id_map = [item[\"report_id\"] for item in reports_data]\n",
    "        batch_size = 128; all_embeddings_list = []\n",
    "        try:\n",
    "            self.model.eval()\n",
    "            for i in tqdm(range(0, len(report_texts), batch_size), desc=\"Embedding Reports (CLIP)\"):\n",
    "                 batch_texts = report_texts[i:i+batch_size]\n",
    "                 inputs = self.tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "                 with torch.no_grad():\n",
    "                     text_features = self.model.get_text_features(**inputs)\n",
    "                     text_features = torch.nn.functional.normalize(text_features, p=2, dim=1)\n",
    "                     all_embeddings_list.append(text_features.cpu())\n",
    "            if not all_embeddings_list: raise ValueError(\"No embeddings generated.\")\n",
    "            embeddings_tensor = torch.cat(all_embeddings_list, dim=0)\n",
    "            print(f\"Generated {embeddings_tensor.shape[0]} CLIP report text embeddings.\")\n",
    "            return embeddings_tensor.numpy().astype('float32')\n",
    "        except Exception as e: print(f\"Error during report text embedding: {e}\"); cleanup_memory(); return None\n",
    "\n",
    "    # --- Added method for single text embedding ---\n",
    "    def create_single_text_embedding(self, text: str):\n",
    "        \"\"\"Generates CLIP embedding for a single piece of text.\"\"\"\n",
    "        if not text: return None\n",
    "        self._load_clip_model()\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            print(\"Error: CLIP model/tokenizer not loaded for single embedding.\")\n",
    "            return None\n",
    "        try:\n",
    "            self.model.eval()\n",
    "            inputs = self.tokenizer([text], padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            with torch.no_grad():\n",
    "                text_features = self.model.get_text_features(**inputs)\n",
    "                text_features = torch.nn.functional.normalize(text_features, p=2, dim=1)\n",
    "            return text_features.cpu().numpy().astype('float32')\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating single text embedding: {e}\")\n",
    "            return None\n",
    "    # --- End Added method ---\n",
    "\n",
    "    def embed_query_image(self, image_path):\n",
    "        \"\"\"Generates query image embedding using CLIP.\"\"\"\n",
    "        self._load_clip_model()\n",
    "        if not self.model or not self.processor: print(\"Error: CLIP model/processor unavailable.\"); return None\n",
    "        if not image_path or not os.path.exists(image_path): print(f\"Error: Invalid image path: {image_path}\"); return None\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            with torch.no_grad():\n",
    "                inputs = self.processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "                image_features = self.model.get_image_features(**inputs)\n",
    "                image_features = torch.nn.functional.normalize(image_features, p=2, dim=1)\n",
    "            return image_features.cpu().numpy().astype('float32')\n",
    "        except Exception as e: print(f\"Error embedding query image {image_path}: {e}\"); return None\n",
    "\n",
    "    def build_faiss_index(self, embeddings):\n",
    "        \"\"\"Builds Faiss index.\"\"\"\n",
    "        if embeddings is None or embeddings.shape[0] == 0: print(\"Error: No embeddings for Faiss.\"); return None\n",
    "        dimension = embeddings.shape[1]; num_embeddings = embeddings.shape[0]\n",
    "        print(f\"Building Faiss index for {num_embeddings} embeddings (Dim: {dimension})...\")\n",
    "        self.faiss_index = faiss.IndexFlatIP(dimension); print(\"Using CPU for Faiss index.\"); self.config[\"faiss_use_gpu\"] = False\n",
    "        faiss.normalize_L2(embeddings); self.faiss_index.add(embeddings); print(f\"Faiss index built. Size: {self.faiss_index.ntotal}\"); return self.faiss_index\n",
    "\n",
    "    def save_index(self, index_path, map_path):\n",
    "        \"\"\"Saves Faiss index and map.\"\"\"\n",
    "        if self.faiss_index and self.report_id_map:\n",
    "            print(f\"Saving Faiss index ({self.faiss_index.ntotal}) to {index_path}\"); faiss.write_index(self.faiss_index, index_path)\n",
    "            print(f\"Saving report ID map ({len(self.report_id_map)}) to {map_path}\"); np.save(map_path, np.array(self.report_id_map, dtype=object))\n",
    "        else: print(\"Index or map empty, nothing to save.\")\n",
    "\n",
    "    def load_index(self, index_path, map_path):\n",
    "        \"\"\"Loads Faiss index and map.\"\"\"\n",
    "        if os.path.exists(index_path) and os.path.exists(map_path):\n",
    "            print(f\"Loading Faiss index from {index_path}\"); self.faiss_index = faiss.read_index(index_path)\n",
    "            print(f\"Loading report ID map from {map_path}\"); self.report_id_map = np.load(map_path, allow_pickle=True).tolist()\n",
    "            print(f\"Loaded index ({self.faiss_index.ntotal}) and map ({len(self.report_id_map)}).\")\n",
    "            expected_dim = 512\n",
    "            if self.faiss_index.d != expected_dim: print(f\"WARNING: Index dim ({self.faiss_index.d}) != expected ({expected_dim}).\")\n",
    "            if self.faiss_index.ntotal != len(self.report_id_map): print(f\"FATAL: Index size != map size.\"); self.faiss_index = None; self.report_id_map = []; return False\n",
    "            print(\"Keeping loaded Faiss index on CPU.\"); self.config[\"faiss_use_gpu\"] = False; return True\n",
    "        else: print(f\"Index/map file not found.\"); return False\n",
    "\n",
    "    def unload_model(self):\n",
    "        \"\"\"Unloads loaded models\"\"\"\n",
    "        if self.model is not None:\n",
    "            print(\"Unloading CLIP AutoModel components...\")\n",
    "            del self.model; del self.processor; del self.tokenizer\n",
    "            self.model = None; self.processor = None; self.tokenizer = None\n",
    "        self.loaded_model_path = None\n",
    "        cleanup_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e8c1c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Retriever:\n",
    "    \"\"\"Retrieves relevant reports using CLIP embeddings.\"\"\"\n",
    "    def __init__(self, embedding_manager, all_reports_data):\n",
    "        self.embed_manager = embedding_manager\n",
    "        self.report_lookup = {item['report_id']: item for item in all_reports_data}\n",
    "        if len(self.embed_manager.report_id_map) > 0 and len(self.report_lookup) != len(self.embed_manager.report_id_map):\n",
    "             print(f\"Retriever Warning: Lookup/map size mismatch. Rebuilding lookup.\"); self.rebuild_lookup(all_reports_data)\n",
    "\n",
    "    def rebuild_lookup(self, all_reports_data):\n",
    "         self.report_lookup = {report_id: next((item for item in all_reports_data if item['report_id'] == report_id), None)\n",
    "                               for report_id in self.embed_manager.report_id_map}\n",
    "         self.report_lookup = {k: v for k, v in self.report_lookup.items() if v is not None}\n",
    "         print(f\"Rebuilt lookup size: {len(self.report_lookup)}\")\n",
    "\n",
    "    def retrieve(self, query_image_embedding, k):\n",
    "        \"\"\"Finds top-k reports using CLIP embeddings (Image to Text).\"\"\"\n",
    "        if self.embed_manager.faiss_index is None: print(\"Error: Faiss index not ready.\"); return [], []\n",
    "        if query_image_embedding is None: print(\"Error: Invalid query embedding.\"); return [], []\n",
    "        if self.embed_manager.faiss_index.ntotal == 0: print(\"Error: Faiss index empty.\"); return [], []\n",
    "        k_actual = min(k, self.embed_manager.faiss_index.ntotal)\n",
    "        faiss.normalize_L2(query_image_embedding)\n",
    "        # print(f\"Searching index ({self.embed_manager.faiss_index.ntotal} items) for top {k_actual} reports...\")\n",
    "        distances, indices = [], []\n",
    "        try:\n",
    "            distances, indices = self.embed_manager.faiss_index.search(query_image_embedding, k_actual)\n",
    "        except Exception as e: print(f\"Error during Faiss search: {e}\"); return [], []\n",
    "        retrieved_reports_data, retrieved_ids = [], []\n",
    "        if len(indices) == 0 or len(distances) == 0 or len(indices[0]) == 0: print(\"Warning: Faiss search returned empty results.\"); return [], []\n",
    "        if indices[0][0] == -1: print(\"Warning: Faiss search returned no valid neighbors (-1 index).\"); return [], []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "             if 0 <= idx < len(self.embed_manager.report_id_map):\n",
    "                 report_id = self.embed_manager.report_id_map[idx]\n",
    "                 if report_id in self.report_lookup:\n",
    "                     report_data = self.report_lookup[report_id].copy()\n",
    "                     report_data['retrieval_score'] = float(distances[0][i])\n",
    "                     retrieved_reports_data.append(report_data); retrieved_ids.append(report_id)\n",
    "        retrieved_reports_data.sort(key=lambda x: x['retrieval_score'], reverse=True)\n",
    "        print(f\"Retrieved {len(retrieved_reports_data)} reports.\")\n",
    "        return retrieved_reports_data, retrieved_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf49f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator:\n",
    "    \"\"\"Generates radiology reports using Ollama.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config; self.model_name = config[\"ollama_generator_model\"] # Use specific key\n",
    "        self.base_url = config.get(\"ollama_base_url\", \"http://localhost:11434\"); self.client = None\n",
    "        self._initialize_client()\n",
    "\n",
    "    def _initialize_client(self):\n",
    "        \"\"\"Initializes Ollama client.\"\"\"\n",
    "        if not OLLAMA_AVAILABLE: print(\"Ollama library not available for Generator.\"); return\n",
    "        try:\n",
    "            print(f\"Initializing Ollama client for GENERATION (Model: '{self.model_name}') at {self.base_url}...\")\n",
    "            self.client = ollama.Client(host=self.base_url); self.client.list(); print(\"Ollama client for GENERATION initialized.\")\n",
    "            available_models = [m['name'] for m in self.client.list()['models']]\n",
    "            if not any(m.startswith(self.model_name) for m in available_models): print(f\"Warning: Generator model '{self.model_name}' not found in Ollama.\")\n",
    "        except Exception as e: print(f\"Error initializing Ollama client for generation: {e}\"); self.client = None\n",
    "\n",
    "    def format_prompt(self, image_path, retrieved_reports, retrieved_triples_map):\n",
    "        \"\"\"Creates text prompt for Ollama Generator.\"\"\"\n",
    "        context_str = \"\"\n",
    "        if retrieved_reports:\n",
    "             context_str += \"Context from similar reports (higher score is more similar):\\n\"\n",
    "             retrieved_reports.sort(key=lambda x: x.get('retrieval_score', -1), reverse=True)\n",
    "             texts = [f\"- {r['report_text'][:120]}...\" for i, r in enumerate(retrieved_reports)]\n",
    "             context_str += \"\\n\".join(texts) + \"\\n\"\n",
    "             # Add triples context\n",
    "             context_str += \"Extracted facts from similar reports:\\n\"\n",
    "             for i, r in enumerate(retrieved_reports):\n",
    "                  triples = retrieved_triples_map.get(r['report_id'], [])\n",
    "                  if triples: context_str += f\"  Report {i+1} Facts: {'; '.join([f'({s}-{p}->{o})' for s, p, o in triples[:5]])}\\n\" # Show top 5\n",
    "\n",
    "        final_prompt = (\n",
    "            f\"{context_str}\\nGiven the provided chest X-ray image, and using the context above (report snippets and extracted facts) internally if helpful, \"\n",
    "            \"generate a radiology report. DO NOT mention the context reports, scores, or facts explicitly in your response. \"\n",
    "            \"The report should contain ONLY a 'Findings:' section and an 'Impression:' section. \"\n",
    "            \"Start the report directly with 'Findings:'.\"\n",
    "        )\n",
    "        return final_prompt\n",
    "\n",
    "    def generate(self, image_path, retrieved_reports, retrieved_triples_map):\n",
    "        \"\"\"Generates report text using Ollama API.\"\"\"\n",
    "        if self.client is None: print(\"Error: Ollama generator client not initialized.\"); return \"Error: Ollama client not available.\"\n",
    "        if not image_path or not os.path.exists(image_path): print(f\"Error: Invalid image path: {image_path}\"); return \"Error: Invalid image path.\"\n",
    "        base64_image = encode_image_to_base64(image_path)\n",
    "        if base64_image is None: return \"Error: Failed to encode image.\"\n",
    "        prompt_text = self.format_prompt(image_path, retrieved_reports, retrieved_triples_map)\n",
    "        print(f\"Sending request to Ollama generator model: {self.model_name}...\"); start_time = time.time()\n",
    "        generated_text = f\"Error: Ollama API call failed.\"\n",
    "        try:\n",
    "            ollama_options = {'num_ctx': self.config.get('ollama_num_ctx', 2048)}\n",
    "            response = self.client.chat(model=self.model_name, messages=[{'role': 'user', 'content': prompt_text, 'images': [base64_image]}], options=ollama_options)\n",
    "            if response and 'message' in response and 'content' in response['message']:\n",
    "                 generated_text = response['message']['content'].strip(); end_time = time.time()\n",
    "                 print(f\"Ollama Generation took {end_time - start_time:.2f} seconds.\")\n",
    "                 print(\"\\n--- Generated Report (Ollama) ---\"); print(generated_text); print(\"---------------------------------\\n\")\n",
    "            else: print(f\"Error: Unexpected response from Ollama: {response}\"); generated_text = \"Error: Unexpected Ollama response.\"\n",
    "        except Exception as e:\n",
    "             print(f\"Error during Ollama API call: {e}\")\n",
    "             if \"connection refused\" in str(e).lower(): print(\">>> Is the Ollama server running? <<<\")\n",
    "             generated_text = f\"Error during Ollama API call: {e}\"\n",
    "        finally: return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ad2ebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Visualization Function ---\n",
    "def create_kg_visualization_html(output_filename, query_info, retrieved_info, generated_info, evaluation_info):\n",
    "    \"\"\"Creates an HTML file with SEPARATE vis.js graph visualizations and evaluation.\"\"\"\n",
    "\n",
    "    html_content = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Radiology RAG Report with KG Visualizations & Evaluation</title>\n",
    "    <script type=\"text/javascript\" src=\"https://unpkg.com/vis-network/standalone/umd/vis-network.min.js\"></script>\n",
    "    <style type=\"text/css\">\n",
    "        body {{ font-family: sans-serif; line-height: 1.6; margin: 20px; }}\n",
    "        .kg-container {{ margin-bottom: 30px; padding-bottom: 20px; border-bottom: 1px solid #ddd; }}\n",
    "        .vis-network {{ width: 95%; height: 450px; border: 1px solid lightgray; margin-top: 10px; }}\n",
    "        h1, h2, h3 {{ border-bottom: 1px solid #eee; padding-bottom: 5px; }}\n",
    "        pre {{ white-space: pre-wrap; word-wrap: break-word; background-color: #f8f8f8; border: 1px solid #ddd; padding: 10px; border-radius: 4px; }}\n",
    "        .evaluation {{ background-color: #eef; padding: 15px; border: 1px solid #dde; border-radius: 5px; margin-top: 20px; }}\n",
    "        .evaluation strong {{ display: inline-block; min-width: 180px; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>RAG Report with KG Visualizations & Evaluation</h1>\n",
    "    <p><strong>Query Report ID:</strong> {query_info.get(\"report_id\", \"N/A\")}</p>\n",
    "    <p><strong>Query Image Path:</strong> {query_info.get(\"image_path\", \"N/A\")}</p>\n",
    "    <div>{query_info.get(\"image_html\", \"<p>Image not available.</p>\")}</div>\n",
    "\n",
    "    <h2>Retrieved Reports</h2>\n",
    "    <ol>\n",
    "\"\"\"\n",
    "    retrieved_reports = retrieved_info.get(\"reports\", [])\n",
    "    if retrieved_reports:\n",
    "        retrieved_reports.sort(key=lambda x: x.get('retrieval_score', -1), reverse=True)\n",
    "        for i, r in enumerate(retrieved_reports):\n",
    "            html_content += f\"<li>ID={r['report_id']}, Score={r.get('retrieval_score', 'N/A'):.4f}<br>Text Snippet: <em>{r.get('report_text', 'N/A')[:150]}...</em></li>\\n\"\n",
    "    else:\n",
    "        html_content += \"<p>No reports retrieved.</p>\\n\"\n",
    "    html_content += \"</ol>\\n\"\n",
    "\n",
    "    html_content += f\"<h2>Generated Report (Ollama: {generated_info.get('model_name', 'N/A')})</h2>\\n\"\n",
    "    html_content += f\"<pre>{generated_info.get('text', 'Error: Generation failed.')}</pre>\\n\"\n",
    "\n",
    "    # --- Add Evaluation Section ---\n",
    "    html_content += \"<hr><h2>Evaluation vs Ground Truth</h2>\\n\"\n",
    "    html_content += \"<div class='evaluation'>\\n\"\n",
    "    html_content += f\"<p><strong>Embedding Similarity (Cosine):</strong> {evaluation_info.get('embedding_similarity', 'N/A'):.4f}</p>\\n\"\n",
    "    html_content += f\"<p><strong>KG Triple Similarity (Jaccard):</strong> {evaluation_info.get('graph_similarity', 'N/A'):.4f}</p>\\n\"\n",
    "    html_content += f\"<p><strong>Combined Similarity Score:</strong> {evaluation_info.get('combined_similarity', 'N/A'):.4f}</p>\\n\"\n",
    "    # Display ground truth text for comparison\n",
    "    html_content += f\"<h3>Ground Truth Report Text:</h3>\\n<pre>{evaluation_info.get('ground_truth_text', 'N/A')}</pre>\\n\"\n",
    "    html_content += \"</div>\\n\"\n",
    "    # --- End Evaluation Section ---\n",
    "\n",
    "\n",
    "    html_content += \"<hr><h2>Knowledge Graph Visualizations</h2>\"\n",
    "\n",
    "    # --- Function to generate graph data and script for one report ---\n",
    "    def generate_graph_section(div_id, title, triples, group_color='blue'):\n",
    "        nodes, edges, node_ids, current_id = [], [], {}, 0\n",
    "        def add_node(name, node_ids, nodes):\n",
    "            nonlocal current_id\n",
    "            name_lower = name.lower()\n",
    "            if name_lower not in node_ids: node_ids[name_lower] = current_id; nodes.append({\"id\": current_id, \"label\": name}); current_id += 1 # No group needed for single graph\n",
    "            return node_ids[name_lower]\n",
    "\n",
    "        if triples:\n",
    "            for subj, pred, obj in triples:\n",
    "                subj_id = add_node(subj, node_ids, nodes)\n",
    "                obj_id = add_node(obj, node_ids, nodes)\n",
    "                edges.append({\"from\": subj_id, \"to\": obj_id, \"label\": pred, \"arrows\": \"to\", \"color\": group_color})\n",
    "        else:\n",
    "             nodes.append({\"id\": 0, \"label\": \"No triples extracted\", \"color\": \"grey\"})\n",
    "\n",
    "        nodes_json = json.dumps(nodes); edges_json = json.dumps(edges)\n",
    "        section_html = f\"\"\"\n",
    "        <div class=\"kg-container\">\n",
    "            <h3>{title}</h3>\n",
    "            <div id=\"{div_id}\" class=\"vis-network\"></div>\n",
    "            <script type=\"text/javascript\">\n",
    "              (function() {{ // IIFE to avoid variable conflicts\n",
    "                var nodes_{div_id} = new vis.DataSet({nodes_json}); var edges_{div_id} = new vis.DataSet({edges_json});\n",
    "                var container_{div_id} = document.getElementById('{div_id}'); var data_{div_id} = {{ nodes: nodes_{div_id}, edges: edges_{div_id} }};\n",
    "                var options_{div_id} = {{ nodes: {{ shape: 'box', size: 16, margin: 10 }}, edges: {{ font: {{ size: 10, align: 'middle' }}, smooth: {{ type: \"continuous\" }} }}, physics: {{ stabilization: {{ iterations: 150 }}, solver: 'repulsion' }}, interaction: {{ tooltipDelay: 200 }} }};\n",
    "                var network_{div_id} = new vis.Network(container_{div_id}, data_{div_id}, options_{div_id});\n",
    "              }})();\n",
    "            </script>\n",
    "        </div>\"\"\"\n",
    "        return section_html\n",
    "\n",
    "    # --- Generate KG for Generated Report ---\n",
    "    html_content += generate_graph_section(div_id=\"gen_network\", title=\"Generated Report KG\",\n",
    "                                           triples=generated_info.get(\"triples\", []), group_color='blue')\n",
    "\n",
    "    # --- Generate KG for Ground Truth Report ---\n",
    "    html_content += generate_graph_section(div_id=\"gt_network\", title=\"Ground Truth Report KG\",\n",
    "                                           triples=evaluation_info.get(\"ground_truth_triples\", []), group_color='purple')\n",
    "\n",
    "\n",
    "    # --- Generate KG for Retrieved Reports ---\n",
    "    retrieved_triples_map = retrieved_info.get(\"triples_map\", {})\n",
    "    retrieved_colors = [\"red\", \"green\", \"orange\"] # Colors for retrieved reports\n",
    "    for i, report_data in enumerate(retrieved_reports):\n",
    "        report_id = report_data[\"report_id\"]; triples = retrieved_triples_map.get(report_id, [])\n",
    "        html_content += generate_graph_section(div_id=f\"retrieved{i+1}_network\", title=f\"Retrieved Report {i+1} (ID: {report_id}) KG\",\n",
    "                                               triples=triples, group_color=retrieved_colors[i % len(retrieved_colors)])\n",
    "\n",
    "    html_content += \"</body>\\n</html>\"\n",
    "\n",
    "    # Save the HTML content\n",
    "    try:\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f: f.write(html_content)\n",
    "        print(f\"Saved HTML report with KG visualization to: {output_filename}\")\n",
    "    except Exception as e: print(f\"Error saving HTML output file {output_filename}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89f17eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Radiology RAG System with KG Vis & Evaluation...\n",
      "\n",
      "--- Stage 1: Loading Data ---\n",
      "Found 393 reports. Processing up to 150...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Reports:  39%|███▉      | 154/393 [00:01<00:01, 146.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 150 reports with front images. Skipped 4.\n",
      "\n",
      "--- Stage 2: Initializing Managers ---\n",
      "EmbeddingManager (CLIP) using device: cuda\n",
      "Initializing Ollama client for Triple Extraction (Model: 'llama3') at http://localhost:11434...\n",
      "Ollama client for Triple Extraction initialized.\n",
      "\n",
      "--- Stage 3: Creating/Loading RAG Index (Base CLIP) ---\n",
      "Using embedding model path for index: openai/clip-vit-base-patch32\n",
      "Loading Faiss index from D:\\NLP apps\\radiology_rag_kg_vis_output_v3\\radiology_clip_index_150.faiss\n",
      "Loading report ID map from D:\\NLP apps\\radiology_rag_kg_vis_output_v3\\radiology_clip_map_150.npy\n",
      "Loaded index (150) and map (150).\n",
      "Keeping loaded Faiss index on CPU.\n",
      "Loaded existing RAG index and map.\n",
      "\n",
      "--- Stage 4: Initializing Retriever ---\n",
      "\n",
      "--- Stage 5: Initializing Ollama Generator ---\n",
      "Initializing Ollama client for GENERATION (Model: 'llava-llama3') at http://localhost:11434...\n",
      "Ollama client for GENERATION initialized.\n",
      "\n",
      "--- Stage 6: Initializing Triple Extractor for RAG ---\n",
      "\n",
      "--- Running Example RAG with KG Extraction, Visualization & Evaluation ---\n",
      "Querying with Report ID: 173, Image: D:\\NLP apps\\Scans\\CXR173_IM-0481-1001.png\n",
      "Loading CLIP model/processor/tokenizer: openai/clip-vit-base-patch32...\n",
      "CLIP components loaded.\n",
      "Retrieved 3 reports.\n",
      "Extracting triples from retrieved reports: ['225', '224', '195']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Retrieved Triples: 100%|██████████| 3/3 [11:07<00:00, 222.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request to Ollama generator model: llava-llama3...\n",
      "Ollama Generation took 65.77 seconds.\n",
      "\n",
      "--- Generated Report (Ollama) ---\n",
      "Findings:\n",
      "- The chest X-ray reveals that the cardiomediastinal silhouette is within normal limits.\n",
      "- There are no visible signs of pneumothorax or pleural effusion.\n",
      "- Focal consolidation and rib fractures were not observed.\n",
      "\n",
      "Impression:\n",
      "The findings indicate a relatively normal chest scan with no significant abnormalities.\n",
      "---------------------------------\n",
      "\n",
      "Extracting triples from generated report...\n",
      "Extracting triples from ground truth report...\n",
      "\n",
      "--- Performing Evaluation vs Ground Truth ---\n",
      "Calculating embedding similarity...\n",
      "  Embedding Similarity: 0.8741\n",
      "Calculating graph similarity...\n",
      "  Graph Similarity (Jaccard): 0.6333\n",
      "  Combined Similarity: 0.8018\n",
      "Unloading models...\n",
      "Unloading CLIP AutoModel components...\n",
      "Saved HTML report with KG visualization to: D:\\NLP apps\\radiology_rag_kg_vis_output_v3\\generated_report_eval_kg_vis_llava-llama3_173.html\n",
      "\n",
      "Radiology RAG+KG Vis System Finished.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Radiology RAG System with KG Vis & Evaluation...\")\n",
    "    os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "    cleanup_memory()\n",
    "\n",
    "    # --- Check Dependencies ---\n",
    "    if not OLLAMA_AVAILABLE: print(\"Ollama not available. Cannot proceed.\") ; exit()\n",
    "\n",
    "    # 1. Load Data\n",
    "    print(\"\\n--- Stage 1: Loading Data ---\")\n",
    "    data_loader = DataLoader(CONFIG[\"report_dir\"], CONFIG[\"scan_dir\"], CONFIG[\"num_reports_to_process\"], CONFIG[\"max_reports_total\"])\n",
    "    all_data = data_loader.load_data();\n",
    "    if not all_data: print(\"No data loaded. Exiting.\"); exit()\n",
    "    # Create a quick lookup for ground truth text by ID\n",
    "    ground_truth_lookup = {item['report_id']: item['report_text'] for item in all_data}\n",
    "\n",
    "\n",
    "    # 2. Initialize Managers\n",
    "    print(\"\\n--- Stage 2: Initializing Managers ---\")\n",
    "    embed_manager = EmbeddingManager(CONFIG) # Handles CLIP embeddings\n",
    "    triple_extractor = TripleExtractor(CONFIG) # Handles Ollama triple extraction\n",
    "    if triple_extractor.client is None: print(\"Triple extractor failed to initialize. Exiting.\"); exit()\n",
    "\n",
    "\n",
    "    # 4. Create/Load RAG Index using BASE CLIP model\n",
    "    print(\"\\n--- Stage 3: Creating/Loading RAG Index (Base CLIP) ---\")\n",
    "    index_file = os.path.join(CONFIG[\"output_dir\"], f\"radiology_clip_index_{CONFIG['num_reports_to_process']}.faiss\")\n",
    "    map_file = os.path.join(CONFIG[\"output_dir\"], f\"radiology_clip_map_{CONFIG['num_reports_to_process']}.npy\")\n",
    "    model_path_for_indexing = CONFIG['embedding_model_name']\n",
    "    print(f\"Using embedding model path for index: {model_path_for_indexing}\")\n",
    "    if not embed_manager.load_index(index_file, map_file):\n",
    "        print(\"Building RAG index from scratch...\")\n",
    "        report_text_embeddings_np = embed_manager.create_report_text_embeddings(all_data)\n",
    "        if report_text_embeddings_np is not None and report_text_embeddings_np.shape[0] > 0:\n",
    "            embed_manager.build_faiss_index(report_text_embeddings_np)\n",
    "            embed_manager.save_index(index_file, map_file)\n",
    "        else: print(\"Failed to create report text embeddings. Cannot build RAG index. Exiting.\"); exit()\n",
    "    else: print(\"Loaded existing RAG index and map.\")\n",
    "\n",
    "    # 5. Initialize Retriever\n",
    "    print(\"\\n--- Stage 4: Initializing Retriever ---\")\n",
    "    retriever = Retriever(embed_manager, all_data)\n",
    "\n",
    "    # 6. Initialize Generator \n",
    "    print(\"\\n--- Stage 5: Initializing Ollama Generator ---\")\n",
    "    generator = None\n",
    "    if OLLAMA_AVAILABLE:\n",
    "        try:\n",
    "            generator = Generator(CONFIG)\n",
    "            if generator.client is None: raise RuntimeError(\"Ollama client failed.\")\n",
    "        except Exception as e: print(f\"Failed to initialize Ollama Generator: {e}\"); generator = None; cleanup_memory()\n",
    "    else: print(\"Ollama library not available. Skipping Generator.\")\n",
    "\n",
    "    # 7. Initialize Triple Extractor (reuse instance)\n",
    "    print(\"\\n--- Stage 6: Initializing Triple Extractor for RAG ---\")\n",
    "    triple_extractor_rag = triple_extractor # Reuse the already initialized one\n",
    "    if not (triple_extractor_rag and triple_extractor_rag.client):\n",
    "         print(\"Triple extractor not available. KG extraction will be skipped.\")\n",
    "\n",
    "\n",
    "    # --- Example RAG Pipeline Execution ---\n",
    "    print(\"\\n--- Running Example RAG with KG Extraction, Visualization & Evaluation ---\")\n",
    "    if not all_data: print(\"No data available.\"); exit()\n",
    "\n",
    "    # --- Select Query Item ---\n",
    "    TARGET_REPORT_ID = \"173\" # <<< CHANGE THIS ID TO TEST A DIFFERENT REPORT, or set to None for random\n",
    "    query_item = None\n",
    "    if TARGET_REPORT_ID:\n",
    "        query_item = next((item for item in all_data if item['report_id'] == TARGET_REPORT_ID), None)\n",
    "        if not query_item:\n",
    "            print(f\"Error: Target report ID '{TARGET_REPORT_ID}' not found. Falling back...\"); TARGET_REPORT_ID = None\n",
    "        elif not query_item.get(\"front_image_path\") or not os.path.exists(query_item[\"front_image_path\"]):\n",
    "             print(f\"Error: Target report ID '{TARGET_REPORT_ID}' image missing. Falling back...\"); TARGET_REPORT_ID = None; query_item = None\n",
    "    if query_item is None: # Fallback if target ID failed or wasn't specified\n",
    "        valid_items = [item for item in all_data if item.get(\"front_image_path\") and os.path.exists(item[\"front_image_path\"])]\n",
    "        if not valid_items: print(\"Could not find any valid query items. Exiting.\"); exit()\n",
    "        query_item = random.choice(valid_items); print(f\"Using report ID: {query_item['report_id']}\")\n",
    "    # --- End Select Query Item ---\n",
    "\n",
    "    query_report_id = query_item[\"report_id\"]; query_image_path = query_item[\"front_image_path\"]\n",
    "    ground_truth_text = ground_truth_lookup.get(query_report_id, \"\") # Get ground truth text\n",
    "    print(f\"Querying with Report ID: {query_report_id}, Image: {query_image_path}\")\n",
    "\n",
    "    # a. Embed query image using CLIP\n",
    "    query_embedding = embed_manager.embed_query_image(query_image_path)\n",
    "\n",
    "    if query_embedding is not None:\n",
    "        # b. Retrieve relevant reports\n",
    "        retrieved_reports_data, retrieved_ids = retriever.retrieve(query_embedding, k=CONFIG[\"top_k_retrieval\"])\n",
    "\n",
    "        # c. Extract triples for retrieved reports (using Ollama Llama3)\n",
    "        retrieved_triples_map = {}\n",
    "        if retrieved_ids and triple_extractor_rag and triple_extractor_rag.client:\n",
    "            print(f\"Extracting triples from retrieved reports: {retrieved_ids}\")\n",
    "            for report_data in tqdm(retrieved_reports_data, desc=\"Extracting Retrieved Triples\"):\n",
    "                triples = triple_extractor_rag.extract_triples(report_data[\"report_text\"])\n",
    "                if triples: retrieved_triples_map[report_data[\"report_id\"]] = triples\n",
    "        else: print(\"Skipping triple extraction for retrieved reports.\")\n",
    "\n",
    "        # d. Generate report (using Ollama LLaVA)\n",
    "        generated_report_text = \"Error: Generator not available.\"\n",
    "        if generator and generator.client:\n",
    "             generated_report_text = generator.generate(query_image_path, retrieved_reports_data, retrieved_triples_map)\n",
    "        else: print(\"Skipping generation as Ollama generator is not available.\")\n",
    "\n",
    "        # e. Extract triples from generated report AND ground truth report\n",
    "        generated_triples = []\n",
    "        ground_truth_triples = []\n",
    "        if not generated_report_text.startswith(\"Error:\") and triple_extractor_rag and triple_extractor_rag.client:\n",
    "            print(\"Extracting triples from generated report...\")\n",
    "            generated_triples = triple_extractor_rag.extract_triples(generated_report_text)\n",
    "        else: print(\"Skipping triple extraction for generated report.\")\n",
    "\n",
    "        if ground_truth_text and triple_extractor_rag and triple_extractor_rag.client:\n",
    "             print(\"Extracting triples from ground truth report...\")\n",
    "             ground_truth_triples = triple_extractor_rag.extract_triples(ground_truth_text)\n",
    "        else: print(\"Skipping triple extraction for ground truth report.\")\n",
    "\n",
    "        # --- f. Perform Evaluation ---\n",
    "        print(\"\\n--- Performing Evaluation vs Ground Truth ---\")\n",
    "        eval_results = {\"embedding_similarity\": 0.0, \"graph_similarity\": 0.0, \"combined_similarity\": 0.0,\n",
    "                        \"ground_truth_text\": ground_truth_text, \"ground_truth_triples\": ground_truth_triples}\n",
    "\n",
    "        if not generated_report_text.startswith(\"Error:\") and ground_truth_text:\n",
    "            # i. Embedding Similarity\n",
    "            print(\"Calculating embedding similarity...\")\n",
    "            gen_embedding = embed_manager.create_single_text_embedding(generated_report_text)\n",
    "            gt_embedding = embed_manager.create_single_text_embedding(ground_truth_text)\n",
    "            if gen_embedding is not None and gt_embedding is not None:\n",
    "                # Cosine similarity between the two 1D vectors\n",
    "                sim = cosine_similarity(gen_embedding, gt_embedding)[0][0]\n",
    "                eval_results[\"embedding_similarity\"] = float(sim) # Ensure float\n",
    "                print(f\"  Embedding Similarity: {sim:.4f}\")\n",
    "            else:\n",
    "                print(\"  Could not calculate embedding similarity (embedding failed).\")\n",
    "                eval_results[\"embedding_similarity\"] = \"Error\"\n",
    "\n",
    "\n",
    "            # ii. Graph (Triple) Similarity\n",
    "            print(\"Calculating graph similarity...\")\n",
    "            graph_sim = calculate_graph_similarity(generated_triples, ground_truth_triples)\n",
    "            eval_results[\"graph_similarity\"] = graph_sim\n",
    "            print(f\"  Graph Similarity (Jaccard): {graph_sim:.4f}\")\n",
    "\n",
    "            # iii. Combined Metric\n",
    "            if isinstance(eval_results[\"embedding_similarity\"], float): # Check if embedding sim was successful\n",
    "                 combined_sim = (CONFIG[\"eval_embedding_weight\"] * eval_results[\"embedding_similarity\"] +\n",
    "                                 CONFIG[\"eval_graph_similarity_weight\"] * eval_results[\"graph_similarity\"])\n",
    "                 eval_results[\"combined_similarity\"] = combined_sim\n",
    "                 print(f\"  Combined Similarity: {combined_sim:.4f}\")\n",
    "            else:\n",
    "                 eval_results[\"combined_similarity\"] = \"Error\"\n",
    "                 print(f\"  Combined Similarity: Error (due to embedding error)\")\n",
    "\n",
    "        else:\n",
    "            print(\"Skipping evaluation because generated report or ground truth is missing/invalid.\")\n",
    "            eval_results = {k: \"N/A\" for k in eval_results} # Set all to N/A\n",
    "            eval_results[\"ground_truth_text\"] = ground_truth_text # Still keep GT text if available\n",
    "            eval_results[\"ground_truth_triples\"] = ground_truth_triples\n",
    "\n",
    "\n",
    "        # Unload models\n",
    "        print(\"Unloading models...\")\n",
    "        if 'triple_extractor_rag' in locals() and triple_extractor_rag: triple_extractor_rag.unload_pipeline()\n",
    "        embed_manager.unload_model()\n",
    "        cleanup_memory()\n",
    "\n",
    "        # g. Prepare data for HTML output\n",
    "        query_info = {\"report_id\": query_report_id, \"image_path\": query_image_path,\n",
    "                      \"image_html\": f'<img src=\"data:image/jpeg;base64,{encode_image_to_base64(query_image_path)}\" alt=\"Query Image {query_report_id}\" style=\"max-width: 400px; height: auto; border: 1px solid #ccc; margin-bottom: 10px;\"><br>' if os.path.exists(query_image_path) else \"<p>Query image not found.</p>\"}\n",
    "        retrieved_info = {\"reports\": retrieved_reports_data, \"triples_map\": retrieved_triples_map}\n",
    "        generated_info = {\"model_name\": CONFIG[\"ollama_generator_model\"], \"text\": generated_report_text, \"triples\": generated_triples}\n",
    "\n",
    "        # h. Create HTML visualization including evaluation\n",
    "        output_filename_html = os.path.join(CONFIG[\"output_dir\"], f\"generated_report_eval_kg_vis_{CONFIG['ollama_generator_model'].replace(':','-')}_{query_report_id}.html\")\n",
    "        create_kg_visualization_html(output_filename_html, query_info, retrieved_info, generated_info, eval_results) # Pass eval results\n",
    "\n",
    "    else: print(\"Failed to generate query image embedding. Cannot proceed.\")\n",
    "\n",
    "    print(\"\\nRadiology RAG+KG Vis System Finished.\")\n",
    "    cleanup_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe716f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative",
   "language": "python",
   "name": "generative"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
