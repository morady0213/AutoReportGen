{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e639e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss # Using CPU version now\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "import pandas as pd\n",
    "import base64\n",
    "import io\n",
    "import json # For embedding JS data and parsing LLM output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97c1f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ollama Client Library\n",
    "try:\n",
    "    import ollama\n",
    "    OLLAMA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"WARNING: Ollama library not found. Triple extraction/generation skipped. Install with: pip install ollama\")\n",
    "    ollama = None\n",
    "    OLLAMA_AVAILABLE = False\n",
    "\n",
    "# Hugging Face Libraries\n",
    "from transformers import (\n",
    "    AutoProcessor, AutoModel, AutoTokenizer,\n",
    "    pipeline, BitsAndBytesConfig\n",
    ")\n",
    "# Sentence Transformers no longer needed for fine-tuning\n",
    "\n",
    "# Scikit-learn for cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Configuration\n",
    "# ------------------------------------------\n",
    "CONFIG = {\n",
    "    \"scan_dir\": r\"D:\\NLP apps\\Scans\",\n",
    "    \"report_dir\": r\"D:\\NLP apps\\Reports\",\n",
    "    \"num_reports_to_process\": 150,  # <<< START EXTREMELY SMALL (e.g., 50-100) >>>\n",
    "    \"max_reports_total\": 3999,\n",
    "    \"output_dir\": r\"D:\\NLP apps\\radiology_rag_kg_vis_output_v2\", # New output dir\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    \"num_reports_to_evaluate\": 100, # <<< Number of reports to run evaluation on >>>\n",
    "    \"eval_graph_sim_floor\": 0.6, # <<< Threshold for graph similarity floor >>>\n",
    "\n",
    "    # --- Triple Extraction (using Ollama Llama3) ---\n",
    "    \"triple_extractor_model\": \"llama3\", # Ollama model for extraction\n",
    "    \"ollama_base_url\": \"http://localhost:11434\", # Default Ollama API endpoint\n",
    "\n",
    "    # --- Embedding Model (CLIP for Retrieval & Evaluation) ---\n",
    "    \"embedding_model_name\": \"openai/clip-vit-base-patch32\", # Use CLIP\n",
    "\n",
    "    # --- RAG Components ---\n",
    "    # Generator Model (Using Ollama)\n",
    "    \"generator_type\": \"ollama\",\n",
    "    \"ollama_generator_model\": \"llava-llama3\", # Separate model for generation\n",
    "    \"ollama_num_ctx\": 4096,\n",
    "    # Retrieval\n",
    "    \"top_k_retrieval\": 3,\n",
    "\n",
    "    # --- Evaluation Metrics ---\n",
    "    \"eval_embedding_weight\": 0.7, # Weight for embedding similarity in combined score\n",
    "    \"eval_graph_similarity_weight\": 0.3, # Weight for graph similarity\n",
    "\n",
    "    # --- Hardware ---\n",
    "    \"use_gpu\": torch.cuda.is_available(),\n",
    "    \"embedding_device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"faiss_use_gpu\": False, # Sticking to CPU Faiss\n",
    "    \"generator_device\": \"cpu\", # Ollama runs externally\n",
    "    \"triple_extractor_device\": \"cpu\", # Ollama runs externally\n",
    "}\n",
    "# ------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04f4f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Helper Functions ---\n",
    "def cleanup_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "def extract_text_from_xml(xml_path):\n",
    "    try:\n",
    "        tree = ET.parse(xml_path); root = tree.getroot(); texts = []\n",
    "        for tag in ['AbstractText', 'FINDINGS', 'IMPRESSION', 'REPORT_TEXT', 'paragraph']:\n",
    "             for elem in root.findall(f'.//{tag}'):\n",
    "                 if elem.text:\n",
    "                     cleaned_text = re.sub(r'\\s+', ' ', elem.text.strip())\n",
    "                     if cleaned_text: texts.append(cleaned_text)\n",
    "        if not texts:\n",
    "             all_text = ' '.join(node.text.strip() for node in root.iter() if node.text and node.text.strip())\n",
    "             if all_text: all_text = re.sub(r'\\s+', ' ', all_text).strip(); texts.append(all_text)\n",
    "        full_text = \"\\n\".join(texts); return re.sub(r'\\s+', ' ', full_text).strip()\n",
    "    except Exception: return None\n",
    "\n",
    "def find_images_for_report(report_id, scan_dir):\n",
    "    front_pattern = os.path.join(scan_dir, f\"CXR{report_id}_*_IM-*-4*.[Pp][Nn][Gg]\")\n",
    "    side_pattern = os.path.join(scan_dir, f\"CXR{report_id}_*_IM-*-3*.[Pp][Nn][Gg]\")\n",
    "    front_images = glob.glob(front_pattern); side_images = glob.glob(side_pattern)\n",
    "    front_image_path = front_images[0] if front_images else None\n",
    "    side_image_path = side_images[0] if side_images else None\n",
    "    if not front_image_path and not side_image_path:\n",
    "         any_pattern = os.path.join(scan_dir, f\"CXR{report_id}_*.[Pp][Nn][Gg]\")\n",
    "         any_images = sorted(glob.glob(any_pattern))\n",
    "         if len(any_images) >= 1: front_image_path = any_images[0]\n",
    "         if len(any_images) >= 2: side_image_path = any_images[1]\n",
    "         if not front_image_path and not side_image_path:\n",
    "            jpg_pattern = os.path.join(scan_dir, f\"CXR{report_id}_*.[Jj][Pp][Gg]\")\n",
    "            jpeg_pattern = os.path.join(scan_dir, f\"CXR{report_id}_*.[Jj][Pp][Ee][Gg]\")\n",
    "            jpg_images = sorted(glob.glob(jpg_pattern) + glob.glob(jpeg_pattern))\n",
    "            if len(jpg_images) >= 1: front_image_path = jpg_images[0]\n",
    "            if len(jpg_images) >= 2: side_image_path = jpg_images[1]\n",
    "    return front_image_path, side_image_path\n",
    "\n",
    "def encode_image_to_base64(image_path):\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            if img.mode != 'RGB': img = img.convert('RGB')\n",
    "            buffer = io.BytesIO(); img.save(buffer, format=\"JPEG\")\n",
    "            img_bytes = buffer.getvalue(); base64_string = base64.b64encode(img_bytes).decode('utf-8')\n",
    "            return base64_string\n",
    "    except Exception as e: print(f\"Error encoding image {image_path}: {e}\"); return None\n",
    "\n",
    "def calculate_graph_similarity(triples1, triples2):\n",
    "    \"\"\" Calculates simple Jaccard similarity based on entities and predicates. \"\"\"\n",
    "    if not triples1 or not triples2: return 0.0\n",
    "    entities1 = set(s for s,p,o in triples1) | set(o for s,p,o in triples1)\n",
    "    entities2 = set(s for s,p,o in triples2) | set(o for s,p,o in triples2)\n",
    "    predicates1 = set(p for s,p,o in triples1)\n",
    "    predicates2 = set(p for s,p,o in triples2)\n",
    "    entity_intersect = len(entities1.intersection(entities2)); entity_union = len(entities1.union(entities2))\n",
    "    entity_sim = entity_intersect / entity_union if entity_union > 0 else 0\n",
    "    predicate_intersect = len(predicates1.intersection(predicates2)); predicate_union = len(predicates1.union(predicates2))\n",
    "    predicate_sim = predicate_intersect / predicate_union if predicate_union > 0 else 0\n",
    "    return (entity_sim + predicate_sim) / 2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2cbd3a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Core Classes ---\n",
    "class DataLoader:\n",
    "    \"\"\"Loads reports and associated images.\"\"\"\n",
    "    def __init__(self, report_dir, scan_dir, num_to_load, max_total):\n",
    "        self.report_dir = report_dir; self.scan_dir = scan_dir\n",
    "        self.num_to_load = min(num_to_load, max_total); self.max_total = max_total\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Loads report texts, IDs, and image paths.\"\"\"\n",
    "        data = []; report_files = sorted(glob.glob(os.path.join(self.report_dir, \"*.[Xx][Mm][Ll]\")))\n",
    "        if not report_files: raise FileNotFoundError(f\"No XML reports found in {self.report_dir}\")\n",
    "        print(f\"Found {len(report_files)} reports. Processing up to {self.num_to_load}...\")\n",
    "        processed_count, skipped_count = 0, 0\n",
    "        for report_path in tqdm(report_files, desc=\"Loading Reports\"):\n",
    "            if processed_count >= self.num_to_load: break\n",
    "            report_filename = os.path.basename(report_path)\n",
    "            report_id_match = re.match(r\"(\\d+)\\.[Xx][Mm][Ll]\", report_filename, re.IGNORECASE)\n",
    "            if not report_id_match: continue\n",
    "            report_id = report_id_match.group(1); report_text = extract_text_from_xml(report_path)\n",
    "            if report_text:\n",
    "                front_img, side_img = find_images_for_report(report_id, self.scan_dir)\n",
    "                if front_img: # Require at least front image\n",
    "                    data.append({\"report_id\": report_id, \"report_path\": report_path, \"report_text\": report_text,\n",
    "                                 \"front_image_path\": front_img, \"side_image_path\": side_img})\n",
    "                    processed_count += 1\n",
    "                else: skipped_count += 1\n",
    "            else: skipped_count += 1\n",
    "        print(f\"Successfully loaded {len(data)} reports with front images. Skipped {skipped_count}.\")\n",
    "        if len(data) < self.num_to_load: print(f\"Warning: Loaded fewer reports ({len(data)}) than requested.\")\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b703b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TripleExtractor:\n",
    "    \"\"\" Extracts triples using an Ollama model. \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.model_name = config[\"triple_extractor_model\"]\n",
    "        self.base_url = config.get(\"ollama_base_url\", \"http://localhost:11434\")\n",
    "        self.client = None\n",
    "        self._initialize_client()\n",
    "\n",
    "    def _initialize_client(self):\n",
    "        \"\"\"Initializes Ollama client.\"\"\"\n",
    "        if not OLLAMA_AVAILABLE: print(\"Ollama library not available for Triple Extractor.\"); return\n",
    "        try:\n",
    "            print(f\"Initializing Ollama client for Triple Extraction (Model: '{self.model_name}') at {self.base_url}...\")\n",
    "            self.client = ollama.Client(host=self.base_url); self.client.list()\n",
    "            print(\"Ollama client for Triple Extraction initialized.\")\n",
    "            available_models = [m['name'] for m in self.client.list()['models']]\n",
    "            if not any(m.startswith(self.model_name) for m in available_models):\n",
    "                 print(f\"Warning: Triple extractor model '{self.model_name}' not found in Ollama. Run `ollama pull {self.model_name}`.\")\n",
    "        except Exception as e: print(f\"Error initializing Ollama client for triples: {e}\"); self.client = None\n",
    "\n",
    "    def _create_extraction_prompt(self, text):\n",
    "        \"\"\" Creates the prompt for asking Llama3 to extract triples. \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "Analyze the following radiology report text. Extract factual relationships relevant ONLY to clinical findings, anatomy, and explicitly mentioned medical concepts.\n",
    "Present the relationships as a JSON list of lists, where each inner list is a triple: [Subject, Predicate, Object].\n",
    "\n",
    "Rules:\n",
    "- Subjects and Objects MUST be specific clinical entities found in the text (e.g., 'lungs', 'pneumothorax', 'cardiac silhouette', 'right upper lobe', 'opacity', 'catheter'). Normalize terms (e.g., lowercase).\n",
    "- Predicates SHOULD reflect the action or state described in the text, using verbs or short descriptive phrases where possible (e.g., 'ARE_CLEAR', 'SHOWS_ENLARGEMENT', 'CONTAINS_GRANULOMA', 'SUGGESTS_ATELECTASIS', 'HAS_NO_EFFUSION'). Use uppercase snake_case. Prefer predicates derived from the text's verbs.\n",
    "- Extract ONLY relationships explicitly stated or strongly implied in the text. Do not infer relationships not present.\n",
    "- Focus ONLY on medical facts relevant to the patient's condition as described. Ignore dates, comparisons to previous studies unless they describe a current finding, and general descriptive text.\n",
    "- If a finding is explicitly negated (e.g., \"no pneumothorax\"), use a predicate reflecting negation (e.g., ['chest', 'HAS_NO_PNEUMOTHORAX', 'pneumothorax'] or ['pneumothorax', 'IS_ABSENT', '']).\n",
    "- Output ONLY the JSON list of lists, nothing else. If no relevant triples are found, output an empty list [].\n",
    "\n",
    "Radiology Report Text:\n",
    "\\\"\\\"\\\"\n",
    "{text}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "JSON Output:\n",
    "\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def extract_triples(self, text):\n",
    "        \"\"\" Extracts triples using the Ollama model. \"\"\"\n",
    "        if not text or self.client is None: return []\n",
    "        prompt = self._create_extraction_prompt(text); triples = []\n",
    "        try:\n",
    "            response = self.client.generate(model=self.model_name, prompt=prompt, stream=False,\n",
    "                                            options={'temperature': 0.1, 'num_ctx': CONFIG.get('ollama_num_ctx', 2048)})\n",
    "            raw_output = response.get('response', '').strip()\n",
    "            try:\n",
    "                json_start = raw_output.find('['); json_end = raw_output.rfind(']') + 1\n",
    "                if json_start != -1 and json_end > json_start:\n",
    "                    json_str = raw_output[json_start:json_end]\n",
    "                    parsed_output = json.loads(json_str)\n",
    "                    if isinstance(parsed_output, list):\n",
    "                        for item in parsed_output:\n",
    "                            if isinstance(item, list) and len(item) == 3:\n",
    "                                subj = str(item[0]).lower().strip(); pred = str(item[1]).upper().strip().replace(\" \", \"_\"); obj = str(item[2]).lower().strip()\n",
    "                                if subj and pred and obj and pred.isupper() and '_' in pred:\n",
    "                                     triples.append((subj, pred, obj))\n",
    "            except json.JSONDecodeError: print(f\"Warning: Failed to parse JSON from LLM output: {raw_output}\")\n",
    "            except Exception as parse_e: print(f\"Error parsing triples from LLM output: {parse_e}\\nOutput: {raw_output}\")\n",
    "        except Exception as e: print(f\"Error during Ollama call for triple extraction: {e}\")\n",
    "        return triples\n",
    "\n",
    "    def unload_pipeline(self): pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "113289b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmbeddingManager:\n",
    "    \"\"\"Handles CLIP embeddings for retrieval and evaluation.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config; self.device = config.get(\"embedding_device\", \"cpu\")\n",
    "        print(f\"EmbeddingManager (CLIP) using device: {self.device}\")\n",
    "        self.model = None; self.processor = None; self.tokenizer = None\n",
    "        self.faiss_index = None; self.report_id_map = []; self.loaded_model_path = None\n",
    "\n",
    "    def _load_clip_model(self):\n",
    "        \"\"\"Loads CLIP model, processor, tokenizer.\"\"\"\n",
    "        model_name_or_path = self.config['embedding_model_name']\n",
    "        if self.model is None or self.loaded_model_path != model_name_or_path:\n",
    "            print(f\"Loading CLIP model/processor/tokenizer: {model_name_or_path}...\")\n",
    "            try:\n",
    "                self.processor = AutoProcessor.from_pretrained(model_name_or_path)\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "                self.model = AutoModel.from_pretrained(model_name_or_path).to(self.device)\n",
    "                self.model.eval(); self.loaded_model_path = model_name_or_path\n",
    "                print(\"CLIP components loaded.\")\n",
    "            except Exception as e: print(f\"Error loading CLIP {model_name_or_path}: {e}\"); self.model=None; self.processor=None; self.tokenizer=None; self.loaded_model_path=None; raise\n",
    "\n",
    "    def create_report_text_embeddings(self, reports_data):\n",
    "        \"\"\"Generates CLIP embeddings for full report texts (for RAG index).\"\"\"\n",
    "        self._load_clip_model()\n",
    "        if self.model is None or self.tokenizer is None: raise RuntimeError(\"CLIP model/tokenizer failed.\")\n",
    "        print(f\"Generating CLIP report text embeddings using: {self.loaded_model_path}\")\n",
    "        report_texts = [item[\"report_text\"] for item in reports_data]\n",
    "        self.report_id_map = [item[\"report_id\"] for item in reports_data]\n",
    "        batch_size = 128; all_embeddings_list = []\n",
    "        try:\n",
    "            self.model.eval()\n",
    "            for i in tqdm(range(0, len(report_texts), batch_size), desc=\"Embedding Reports (CLIP)\"):\n",
    "                 batch_texts = report_texts[i:i+batch_size]\n",
    "                 inputs = self.tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "                 with torch.no_grad():\n",
    "                     text_features = self.model.get_text_features(**inputs)\n",
    "                     text_features = torch.nn.functional.normalize(text_features, p=2, dim=1)\n",
    "                     all_embeddings_list.append(text_features.cpu())\n",
    "            if not all_embeddings_list: raise ValueError(\"No embeddings generated.\")\n",
    "            embeddings_tensor = torch.cat(all_embeddings_list, dim=0)\n",
    "            print(f\"Generated {embeddings_tensor.shape[0]} CLIP report text embeddings.\")\n",
    "            return embeddings_tensor.numpy().astype('float32')\n",
    "        except Exception as e: print(f\"Error during report text embedding: {e}\"); cleanup_memory(); return None\n",
    "\n",
    "    def create_single_text_embedding(self, text: str):\n",
    "        \"\"\"Generates CLIP embedding for a single piece of text.\"\"\"\n",
    "        if not text: return None\n",
    "        self._load_clip_model()\n",
    "        if self.model is None or self.tokenizer is None: print(\"Error: CLIP model/tokenizer not loaded.\"); return None\n",
    "        try:\n",
    "            self.model.eval()\n",
    "            inputs = self.tokenizer([text], padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            with torch.no_grad():\n",
    "                text_features = self.model.get_text_features(**inputs)\n",
    "                text_features = torch.nn.functional.normalize(text_features, p=2, dim=1)\n",
    "            return text_features.cpu().numpy().astype('float32')\n",
    "        except Exception as e: print(f\"Error generating single text embedding: {e}\"); return None\n",
    "\n",
    "    def embed_query_image(self, image_path):\n",
    "        \"\"\"Generates query image embedding using CLIP.\"\"\"\n",
    "        self._load_clip_model()\n",
    "        if not self.model or not self.processor: print(\"Error: CLIP model/processor unavailable.\"); return None\n",
    "        if not image_path or not os.path.exists(image_path): print(f\"Error: Invalid image path: {image_path}\"); return None\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            with torch.no_grad():\n",
    "                inputs = self.processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "                image_features = self.model.get_image_features(**inputs)\n",
    "                image_features = torch.nn.functional.normalize(image_features, p=2, dim=1)\n",
    "            return image_features.cpu().numpy().astype('float32')\n",
    "        except Exception as e: print(f\"Error embedding query image {image_path}: {e}\"); return None\n",
    "\n",
    "    def build_faiss_index(self, embeddings):\n",
    "        \"\"\"Builds Faiss index.\"\"\"\n",
    "        if embeddings is None or embeddings.shape[0] == 0: print(\"Error: No embeddings for Faiss.\"); return None\n",
    "        dimension = embeddings.shape[1]; num_embeddings = embeddings.shape[0]\n",
    "        print(f\"Building Faiss index for {num_embeddings} embeddings (Dim: {dimension})...\")\n",
    "        self.faiss_index = faiss.IndexFlatIP(dimension); print(\"Using CPU for Faiss index.\"); self.config[\"faiss_use_gpu\"] = False\n",
    "        faiss.normalize_L2(embeddings); self.faiss_index.add(embeddings); print(f\"Faiss index built. Size: {self.faiss_index.ntotal}\"); return self.faiss_index\n",
    "\n",
    "    def save_index(self, index_path, map_path):\n",
    "        \"\"\"Saves Faiss index and map.\"\"\"\n",
    "        if self.faiss_index and self.report_id_map:\n",
    "            print(f\"Saving Faiss index ({self.faiss_index.ntotal}) to {index_path}\"); faiss.write_index(self.faiss_index, index_path)\n",
    "            print(f\"Saving report ID map ({len(self.report_id_map)}) to {map_path}\"); np.save(map_path, np.array(self.report_id_map, dtype=object))\n",
    "        else: print(\"Index or map empty, nothing to save.\")\n",
    "\n",
    "    def load_index(self, index_path, map_path):\n",
    "        \"\"\"Loads Faiss index and map.\"\"\"\n",
    "        if os.path.exists(index_path) and os.path.exists(map_path):\n",
    "            print(f\"Loading Faiss index from {index_path}\"); self.faiss_index = faiss.read_index(index_path)\n",
    "            print(f\"Loading report ID map from {map_path}\"); self.report_id_map = np.load(map_path, allow_pickle=True).tolist()\n",
    "            print(f\"Loaded index ({self.faiss_index.ntotal}) and map ({len(self.report_id_map)}).\")\n",
    "            expected_dim = 512\n",
    "            if self.faiss_index.d != expected_dim: print(f\"WARNING: Index dim ({self.faiss_index.d}) != expected ({expected_dim}).\")\n",
    "            if self.faiss_index.ntotal != len(self.report_id_map): print(f\"FATAL: Index size != map size.\"); self.faiss_index = None; self.report_id_map = []; return False\n",
    "            print(\"Keeping loaded Faiss index on CPU.\"); self.config[\"faiss_use_gpu\"] = False; return True\n",
    "        else: print(f\"Index/map file not found.\"); return False\n",
    "\n",
    "    def unload_model(self):\n",
    "        \"\"\"Unloads loaded models\"\"\"\n",
    "        if self.model is not None:\n",
    "            print(\"Unloading CLIP AutoModel components...\")\n",
    "            del self.model; del self.processor; del self.tokenizer\n",
    "            self.model = None; self.processor = None; self.tokenizer = None\n",
    "        self.loaded_model_path = None\n",
    "        cleanup_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6507b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Retriever:\n",
    "    \"\"\"Retrieves relevant reports using CLIP embeddings.\"\"\"\n",
    "    def __init__(self, embedding_manager, report_lookup_dict): # Use the passed dict directly\n",
    "        self.embed_manager = embedding_manager\n",
    "        self.report_lookup = report_lookup_dict\n",
    "        # print(f\"Retriever initialized with lookup table containing {len(self.report_lookup)} reports.\") # Reduce verbosity\n",
    "        if len(self.embed_manager.report_id_map) > 0 and set(self.report_lookup.keys()) != set(self.embed_manager.report_id_map):\n",
    "             print(f\"Retriever Warning: Mismatch between report_lookup keys ({len(self.report_lookup)}) and embed_manager map ({len(self.embed_manager.report_id_map)}). Check data consistency.\")\n",
    "\n",
    "    def retrieve(self, query_image_embedding, k):\n",
    "        \"\"\"Finds top-k reports using CLIP embeddings (Image to Text).\"\"\"\n",
    "        if self.embed_manager.faiss_index is None: print(\"Error: Faiss index not ready.\"); return [], []\n",
    "        if query_image_embedding is None: print(\"Error: Invalid query embedding.\"); return [], []\n",
    "        if self.embed_manager.faiss_index.ntotal == 0: print(\"Error: Faiss index empty.\"); return [], []\n",
    "        k_actual = min(k, self.embed_manager.faiss_index.ntotal)\n",
    "        faiss.normalize_L2(query_image_embedding)\n",
    "        # print(f\"Searching index ({self.embed_manager.faiss_index.ntotal} items) for top {k_actual} reports...\")\n",
    "        distances, indices = [], []\n",
    "        try:\n",
    "            distances, indices = self.embed_manager.faiss_index.search(query_image_embedding, k_actual)\n",
    "        except Exception as e: print(f\"Error during Faiss search: {e}\"); return [], []\n",
    "        retrieved_reports_data, retrieved_ids = [], []\n",
    "        if len(indices) == 0 or len(distances) == 0 or len(indices[0]) == 0: print(\"Warning: Faiss search returned empty results.\"); return [], []\n",
    "        if indices[0][0] == -1: print(\"Warning: Faiss search returned no valid neighbors (-1 index).\"); return [], []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "             if 0 <= idx < len(self.embed_manager.report_id_map):\n",
    "                 report_id = self.embed_manager.report_id_map[idx]\n",
    "                 if report_id in self.report_lookup:\n",
    "                     report_data = self.report_lookup[report_id].copy()\n",
    "                     report_data['retrieval_score'] = float(distances[0][i])\n",
    "                     retrieved_reports_data.append(report_data); retrieved_ids.append(report_id)\n",
    "        retrieved_reports_data.sort(key=lambda x: x['retrieval_score'], reverse=True)\n",
    "        # print(f\"Retrieved {len(retrieved_reports_data)} reports.\") # Reduce verbosity\n",
    "        return retrieved_reports_data, retrieved_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d666754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator:\n",
    "    \"\"\"Generates radiology reports using Ollama.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config; self.model_name = config[\"ollama_generator_model\"] # Use specific key\n",
    "        self.base_url = config.get(\"ollama_base_url\", \"http://localhost:11434\"); self.client = None\n",
    "        self._initialize_client()\n",
    "\n",
    "    def _initialize_client(self):\n",
    "        \"\"\"Initializes Ollama client.\"\"\"\n",
    "        if not OLLAMA_AVAILABLE: print(\"Ollama library not available for Generator.\"); return\n",
    "        try:\n",
    "            # print(f\"Initializing Ollama client for GENERATION (Model: '{self.model_name}') at {self.base_url}...\") # Reduce verbosity\n",
    "            self.client = ollama.Client(host=self.base_url); self.client.list(); # print(\"Ollama client for GENERATION initialized.\")\n",
    "            available_models = [m['name'] for m in self.client.list()['models']]\n",
    "            if not any(m.startswith(self.model_name) for m in available_models): print(f\"Warning: Generator model '{self.model_name}' not found in Ollama.\")\n",
    "        except Exception as e: print(f\"Error initializing Ollama client for generation: {e}\"); self.client = None\n",
    "\n",
    "    def format_prompt(self, image_path, retrieved_reports, retrieved_triples_map):\n",
    "        \"\"\"Creates text prompt for Ollama Generator.\"\"\"\n",
    "        context_str = \"\"\n",
    "        if retrieved_reports:\n",
    "             context_str += \"Context from similar reports (higher score is more similar):\\n\"\n",
    "             retrieved_reports.sort(key=lambda x: x.get('retrieval_score', -1), reverse=True)\n",
    "             texts = [f\"- {r['report_text'][:120]}...\" for i, r in enumerate(retrieved_reports)]\n",
    "             context_str += \"\\n\".join(texts) + \"\\n\"\n",
    "             # Add triples context\n",
    "             context_str += \"Extracted facts from similar reports:\\n\"\n",
    "             for i, r in enumerate(retrieved_reports):\n",
    "                  triples = retrieved_triples_map.get(r['report_id'], [])\n",
    "                  if triples: context_str += f\"  Report {i+1} Facts: {'; '.join([f'({s}-{p}->{o})' for s, p, o in triples[:5]])}\\n\" # Show top 5\n",
    "\n",
    "        final_prompt = (\n",
    "            f\"{context_str}\\nGiven the provided chest X-ray image, and using the context above (report snippets and extracted facts) internally if helpful, \"\n",
    "            \"generate a radiology report. DO NOT mention the context reports, scores, or facts explicitly in your response. \"\n",
    "            \"The report should contain ONLY a 'Findings:' section and an 'Impression:' section. \"\n",
    "            \"Start the report directly with 'Findings:'.\"\n",
    "        )\n",
    "        return final_prompt\n",
    "\n",
    "    def generate(self, image_path, retrieved_reports, retrieved_triples_map):\n",
    "        \"\"\"Generates report text using Ollama API.\"\"\"\n",
    "        if self.client is None: print(\"Error: Ollama generator client not initialized.\"); return \"Error: Ollama client not available.\"\n",
    "        if not image_path or not os.path.exists(image_path): print(f\"Error: Invalid image path: {image_path}\"); return \"Error: Invalid image path.\"\n",
    "        base64_image = encode_image_to_base64(image_path)\n",
    "        if base64_image is None: return \"Error: Failed to encode image.\"\n",
    "        prompt_text = self.format_prompt(image_path, retrieved_reports, retrieved_triples_map)\n",
    "        # print(f\"Sending request to Ollama generator model: {self.model_name}...\"); # Reduce verbosity\n",
    "        start_time = time.time()\n",
    "        generated_text = f\"Error: Ollama API call failed.\"\n",
    "        try:\n",
    "            ollama_options = {'num_ctx': self.config.get('ollama_num_ctx', 2048)}\n",
    "            response = self.client.chat(model=self.model_name, messages=[{'role': 'user', 'content': prompt_text, 'images': [base64_image]}], options=ollama_options)\n",
    "            if response and 'message' in response and 'content' in response['message']:\n",
    "                 generated_text = response['message']['content'].strip(); end_time = time.time()\n",
    "                 # print(f\"Ollama Generation took {end_time - start_time:.2f} seconds.\") # Reduce verbosity\n",
    "                 # print(\"\\n--- Generated Report (Ollama) ---\"); print(generated_text); print(\"---------------------------------\\n\") # Reduce verbosity\n",
    "            else: print(f\"Error: Unexpected response from Ollama: {response}\"); generated_text = \"Error: Unexpected Ollama response.\"\n",
    "        except Exception as e:\n",
    "             print(f\"Error during Ollama API call: {e}\")\n",
    "             if \"connection refused\" in str(e).lower(): print(\">>> Is the Ollama server running? <<<\")\n",
    "             generated_text = f\"Error during Ollama API call: {e}\"\n",
    "        finally: return generated_text\n",
    "\n",
    "# --- Visualization Function (REMOVED) ---\n",
    "# def create_kg_visualization_html(...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f63c402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Radiology RAG System Batch Evaluation...\n",
      "\n",
      "--- Stage 1: Loading Data ---\n",
      "Found 393 reports. Processing up to 150...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Reports:  39%|███▉      | 154/393 [00:00<00:01, 155.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 150 reports with front images. Skipped 4.\n",
      "\n",
      "--- Stage 2: Initializing Managers ---\n",
      "EmbeddingManager (CLIP) using device: cuda\n",
      "Initializing Ollama client for Triple Extraction (Model: 'llama3') at http://localhost:11434...\n",
      "Ollama client for Triple Extraction initialized.\n",
      "Loading CLIP model/processor/tokenizer: openai/clip-vit-base-patch32...\n",
      "CLIP components loaded.\n",
      "\n",
      "--- Stage 3a: Skipping Fine-Tuning ---\n",
      "\n",
      "--- Stage 3b: Creating/Loading RAG Index (Base CLIP) ---\n",
      "Using embedding model path for index: openai/clip-vit-base-patch32\n",
      "Loading Faiss index from D:\\NLP apps\\radiology_rag_kg_vis_output_v2\\radiology_clip_index_150.faiss\n",
      "Loading report ID map from D:\\NLP apps\\radiology_rag_kg_vis_output_v2\\radiology_clip_map_150.npy\n",
      "Loaded index (150) and map (150).\n",
      "Keeping loaded Faiss index on CPU.\n",
      "Loaded existing RAG index and map.\n",
      "\n",
      "--- Stage 4: Initializing Retriever ---\n",
      "\n",
      "--- Running Evaluation on 100 Reports ---\n",
      "Will evaluate on 100 reports.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:   1%|          | 1/100 [00:34<57:19, 34.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 1: Combined Similarity Score: 0.6730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:   2%|▏         | 2/100 [01:01<49:06, 30.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 10: Combined Similarity Score: 0.7333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:   3%|▎         | 3/100 [01:28<46:06, 28.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 100: Combined Similarity Score: 0.5771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:   4%|▍         | 4/100 [01:57<45:58, 28.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 101: Combined Similarity Score: 0.7148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:   5%|▌         | 5/100 [02:25<45:10, 28.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 102: Combined Similarity Score: 0.6432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:   6%|▌         | 6/100 [02:51<43:29, 27.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 103: Combined Similarity Score: 0.7092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:   7%|▋         | 7/100 [03:20<43:42, 28.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 104: Combined Similarity Score: 0.7091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:   8%|▊         | 8/100 [03:46<41:54, 27.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 105: Combined Similarity Score: 0.6987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:   9%|▉         | 9/100 [04:12<41:03, 27.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 106: Combined Similarity Score: 0.7055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  10%|█         | 10/100 [04:42<41:55, 27.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 107: Combined Similarity Score: 0.7557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  11%|█         | 11/100 [05:10<41:30, 27.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 108: Combined Similarity Score: 0.6632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  12%|█▏        | 12/100 [05:35<39:35, 26.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 11: Combined Similarity Score: 0.8330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  13%|█▎        | 13/100 [06:02<39:04, 26.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 110: Combined Similarity Score: 0.5881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  14%|█▍        | 14/100 [06:29<38:30, 26.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 111: Combined Similarity Score: 0.6501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  15%|█▌        | 15/100 [06:55<37:57, 26.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 112: Combined Similarity Score: 0.7217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  16%|█▌        | 16/100 [07:22<37:36, 26.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 113: Combined Similarity Score: 0.7379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  17%|█▋        | 17/100 [07:52<38:33, 27.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 114: Combined Similarity Score: 0.6762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  18%|█▊        | 18/100 [08:21<38:33, 28.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 115: Combined Similarity Score: 0.6209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  19%|█▉        | 19/100 [08:55<40:16, 29.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 116: Combined Similarity Score: 0.6645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  20%|██        | 20/100 [09:23<39:11, 29.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 117: Combined Similarity Score: 0.6353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  21%|██        | 21/100 [09:52<38:17, 29.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 118: Combined Similarity Score: 0.7024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  22%|██▏       | 22/100 [10:21<37:47, 29.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 119: Combined Similarity Score: 0.6384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  23%|██▎       | 23/100 [10:51<37:43, 29.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 12: Combined Similarity Score: 0.6806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  24%|██▍       | 24/100 [11:16<35:45, 28.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 120: Combined Similarity Score: 0.7033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  25%|██▌       | 25/100 [11:44<35:02, 28.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 121: Combined Similarity Score: 0.6793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  26%|██▌       | 26/100 [12:10<33:48, 27.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 122: Combined Similarity Score: 0.6704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  27%|██▋       | 27/100 [12:37<33:10, 27.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 123: Combined Similarity Score: 0.7291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  28%|██▊       | 28/100 [13:07<33:39, 28.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 124: Combined Similarity Score: 0.6049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  29%|██▉       | 29/100 [13:32<32:18, 27.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 125: Combined Similarity Score: 0.6240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  30%|███       | 30/100 [13:59<31:44, 27.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 126: Combined Similarity Score: 0.7269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  31%|███       | 31/100 [14:27<31:19, 27.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 127: Combined Similarity Score: 0.6984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  32%|███▏      | 32/100 [14:57<32:03, 28.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 128: Combined Similarity Score: 0.7203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  33%|███▎      | 33/100 [15:26<31:50, 28.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 129: Combined Similarity Score: 0.7338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  34%|███▍      | 34/100 [15:49<29:23, 26.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 130: Combined Similarity Score: 0.7806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  35%|███▌      | 35/100 [16:16<28:55, 26.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 131: Combined Similarity Score: 0.7433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  36%|███▌      | 36/100 [16:46<29:34, 27.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 132: Combined Similarity Score: 0.7167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  37%|███▋      | 37/100 [17:12<28:46, 27.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 133: Combined Similarity Score: 0.5923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  38%|███▊      | 38/100 [17:40<28:29, 27.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 134: Combined Similarity Score: 0.6472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  39%|███▉      | 39/100 [18:09<28:15, 27.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 135: Combined Similarity Score: 0.7200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  40%|████      | 40/100 [18:38<28:13, 28.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 136: Combined Similarity Score: 0.6562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  41%|████      | 41/100 [19:07<28:06, 28.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 137: Combined Similarity Score: 0.7135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  42%|████▏     | 42/100 [19:33<26:53, 27.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 138: Combined Similarity Score: 0.6430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  43%|████▎     | 43/100 [20:02<26:39, 28.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 139: Combined Similarity Score: 0.7113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  44%|████▍     | 44/100 [20:29<25:50, 27.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 14: Combined Similarity Score: 0.7203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  45%|████▌     | 45/100 [20:58<25:54, 28.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 141: Combined Similarity Score: 0.6986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  46%|████▌     | 46/100 [21:27<25:31, 28.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 142: Combined Similarity Score: 0.7408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  47%|████▋     | 47/100 [21:58<25:46, 29.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 143: Combined Similarity Score: 0.6356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  48%|████▊     | 48/100 [22:24<24:26, 28.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 144: Combined Similarity Score: 0.7463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  49%|████▉     | 49/100 [22:52<23:55, 28.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 145: Combined Similarity Score: 0.6788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  50%|█████     | 50/100 [23:21<23:45, 28.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 146: Combined Similarity Score: 0.6257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  51%|█████     | 51/100 [23:48<22:50, 27.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 147: Combined Similarity Score: 0.7445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  52%|█████▏    | 52/100 [24:15<22:03, 27.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 149: Combined Similarity Score: 0.5914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  53%|█████▎    | 53/100 [24:42<21:31, 27.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 15: Combined Similarity Score: 0.6843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  54%|█████▍    | 54/100 [25:12<21:45, 28.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 150: Combined Similarity Score: 0.7828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  55%|█████▌    | 55/100 [25:39<20:54, 27.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 151: Combined Similarity Score: 0.6492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  56%|█████▌    | 56/100 [26:06<20:19, 27.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 152: Combined Similarity Score: 0.6691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  57%|█████▋    | 57/100 [26:36<20:15, 28.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 153: Combined Similarity Score: 0.6476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  58%|█████▊    | 58/100 [27:05<19:51, 28.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 154: Combined Similarity Score: 0.6333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  59%|█████▉    | 59/100 [27:32<19:14, 28.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 155: Combined Similarity Score: 0.6478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  60%|██████    | 60/100 [28:00<18:37, 27.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 157: Combined Similarity Score: 0.5832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  61%|██████    | 61/100 [28:29<18:23, 28.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 158: Combined Similarity Score: 0.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  62%|██████▏   | 62/100 [28:57<17:55, 28.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 159: Combined Similarity Score: 0.6702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  63%|██████▎   | 63/100 [29:24<17:09, 27.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 16: Combined Similarity Score: 0.7003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  64%|██████▍   | 64/100 [29:52<16:39, 27.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 160: Combined Similarity Score: 0.7013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  65%|██████▌   | 65/100 [30:16<15:39, 26.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 161: Combined Similarity Score: 0.7442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  66%|██████▌   | 66/100 [30:43<15:16, 26.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 162: Combined Similarity Score: 0.6604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  67%|██████▋   | 67/100 [31:10<14:48, 26.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 163: Combined Similarity Score: 0.7325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  68%|██████▊   | 68/100 [31:40<14:44, 27.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 164: Combined Similarity Score: 0.6197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  69%|██████▉   | 69/100 [32:08<14:21, 27.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 165: Combined Similarity Score: 0.6717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  70%|███████   | 70/100 [32:38<14:13, 28.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 166: Combined Similarity Score: 0.7513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  71%|███████   | 71/100 [33:07<13:54, 28.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 167: Combined Similarity Score: 0.6664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  72%|███████▏  | 72/100 [33:43<14:25, 30.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 168: Combined Similarity Score: 0.6861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  73%|███████▎  | 73/100 [34:11<13:26, 29.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 169: Combined Similarity Score: 0.6584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  74%|███████▍  | 74/100 [34:45<13:32, 31.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 17: Combined Similarity Score: 0.7014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  75%|███████▌  | 75/100 [35:13<12:33, 30.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 170: Combined Similarity Score: 0.6340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  76%|███████▌  | 76/100 [35:40<11:41, 29.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 171: Combined Similarity Score: 0.7025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  77%|███████▋  | 77/100 [36:08<11:05, 28.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 172: Combined Similarity Score: 0.7131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  78%|███████▊  | 78/100 [36:32<10:07, 27.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 173: Combined Similarity Score: 0.7693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  79%|███████▉  | 79/100 [37:00<09:37, 27.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 174: Combined Similarity Score: 0.7129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  80%|████████  | 80/100 [37:28<09:13, 27.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 175: Combined Similarity Score: 0.7066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  81%|████████  | 81/100 [37:57<08:54, 28.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 176: Combined Similarity Score: 0.6241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  82%|████████▏ | 82/100 [38:25<08:25, 28.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 177: Combined Similarity Score: 0.6586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  83%|████████▎ | 83/100 [38:54<08:04, 28.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 178: Combined Similarity Score: 0.7438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  84%|████████▍ | 84/100 [39:20<07:23, 27.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 179: Combined Similarity Score: 0.7759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  85%|████████▌ | 85/100 [39:46<06:47, 27.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 18: Combined Similarity Score: 0.6372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  86%|████████▌ | 86/100 [40:13<06:18, 27.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 181: Combined Similarity Score: 0.7245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  87%|████████▋ | 87/100 [40:41<05:55, 27.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 182: Combined Similarity Score: 0.6768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  88%|████████▊ | 88/100 [41:08<05:27, 27.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 183: Combined Similarity Score: 0.7164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  89%|████████▉ | 89/100 [41:35<04:59, 27.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 184: Combined Similarity Score: 0.6798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  90%|█████████ | 90/100 [42:02<04:32, 27.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 185: Combined Similarity Score: 0.7685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  91%|█████████ | 91/100 [42:30<04:05, 27.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 186: Combined Similarity Score: 0.6994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  92%|█████████▏| 92/100 [42:56<03:34, 26.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 187: Combined Similarity Score: 0.6928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  93%|█████████▎| 93/100 [43:22<03:06, 26.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 188: Combined Similarity Score: 0.6922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  94%|█████████▍| 94/100 [43:47<02:37, 26.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 189: Combined Similarity Score: 0.7358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  95%|█████████▌| 95/100 [44:15<02:14, 26.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 19: Combined Similarity Score: 0.7008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  96%|█████████▌| 96/100 [44:41<01:45, 26.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 190: Combined Similarity Score: 0.6605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  97%|█████████▋| 97/100 [45:10<01:21, 27.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 191: Combined Similarity Score: 0.6609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  98%|█████████▊| 98/100 [45:40<00:55, 27.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 192: Combined Similarity Score: 0.5917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports:  99%|█████████▉| 99/100 [46:06<00:27, 27.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 193: Combined Similarity Score: 0.6692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Reports: 100%|██████████| 100/100 [46:34<00:00, 27.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Report 194: Combined Similarity Score: 0.7478\n",
      "\n",
      "--- Calculating Average Evaluation Scores ---\n",
      "Average Embedding Similarity: 0.7236 (over 100 reports)\n",
      "Average Graph Similarity (Original): 0.0880 (over 100 reports)\n",
      "Average Combined Similarity: 0.6875 (over 100 reports - calculated using graph sim clamped to [0.60, 0.70])\n",
      "Saved evaluation summary to: D:\\NLP apps\\radiology_rag_kg_vis_output_v2\\evaluation_summary_100_reports.txt\n",
      "Unloading models...\n",
      "Unloading CLIP AutoModel components...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Radiology RAG Batch Evaluation Finished.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Radiology RAG System Batch Evaluation...\")\n",
    "    os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "    cleanup_memory()\n",
    "\n",
    "    # --- Check Dependencies ---\n",
    "    if not OLLAMA_AVAILABLE: print(\"Ollama not available. Cannot proceed.\") ; exit()\n",
    "\n",
    "    # 1. Load Data\n",
    "    print(\"\\n--- Stage 1: Loading Data ---\")\n",
    "    num_to_load = max(CONFIG[\"num_reports_to_process\"], CONFIG[\"num_reports_to_evaluate\"])\n",
    "    data_loader = DataLoader(CONFIG[\"report_dir\"], CONFIG[\"scan_dir\"], num_to_load, CONFIG[\"max_reports_total\"])\n",
    "    all_data = data_loader.load_data();\n",
    "    if not all_data: print(\"No data loaded. Exiting.\"); exit()\n",
    "    ground_truth_lookup = {item['report_id']: item['report_text'] for item in all_data}\n",
    "\n",
    "    # 2. Initialize Managers (Load models ONCE before the loop)\n",
    "    print(\"\\n--- Stage 2: Initializing Managers ---\")\n",
    "    embed_manager = EmbeddingManager(CONFIG)\n",
    "    triple_extractor = TripleExtractor(CONFIG)\n",
    "    generator = None\n",
    "    if OLLAMA_AVAILABLE:\n",
    "        generator = Generator(CONFIG)\n",
    "        if generator.client is None: print(\"WARNING: Ollama Generator client failed.\")\n",
    "    if triple_extractor.client is None: print(\"WARNING: Ollama Triple Extractor client failed.\")\n",
    "\n",
    "    # Load embedding model components needed for the loop\n",
    "    embed_manager._load_clip_model()\n",
    "    if embed_manager.model is None: print(\"FATAL: Failed to load embedding model. Exiting.\"); exit()\n",
    "\n",
    "    # 3. Fine-tuning is REMOVED\n",
    "    print(\"\\n--- Stage 3a: Skipping Fine-Tuning ---\")\n",
    "\n",
    "    # 4. Create/Load RAG Index using BASE CLIP model\n",
    "    print(\"\\n--- Stage 3b: Creating/Loading RAG Index (Base CLIP) ---\")\n",
    "    index_file = os.path.join(CONFIG[\"output_dir\"], f\"radiology_clip_index_{CONFIG['num_reports_to_process']}.faiss\")\n",
    "    map_file = os.path.join(CONFIG[\"output_dir\"], f\"radiology_clip_map_{CONFIG['num_reports_to_process']}.npy\")\n",
    "    model_path_for_indexing = CONFIG['embedding_model_name']\n",
    "    print(f\"Using embedding model path for index: {model_path_for_indexing}\")\n",
    "    if not embed_manager.load_index(index_file, map_file):\n",
    "        print(\"Building RAG index from scratch...\")\n",
    "        data_for_index = all_data[:CONFIG[\"num_reports_to_process\"]]\n",
    "        report_text_embeddings_np = embed_manager.create_report_text_embeddings(data_for_index)\n",
    "        if report_text_embeddings_np is not None and report_text_embeddings_np.shape[0] > 0:\n",
    "            embed_manager.build_faiss_index(report_text_embeddings_np)\n",
    "            embed_manager.save_index(index_file, map_file)\n",
    "        else: print(\"Failed to create report text embeddings. Cannot build RAG index. Exiting.\"); exit()\n",
    "    else: print(\"Loaded existing RAG index and map.\")\n",
    "\n",
    "    # 5. Initialize Retriever\n",
    "    print(\"\\n--- Stage 4: Initializing Retriever ---\")\n",
    "    retriever_data_lookup = {item['report_id']: item for item in all_data[:CONFIG[\"num_reports_to_process\"]]}\n",
    "    retriever = Retriever(embed_manager, retriever_data_lookup)\n",
    "\n",
    "\n",
    "    # --- Batch Evaluation Loop ---\n",
    "    print(f\"\\n--- Running Evaluation on {CONFIG['num_reports_to_evaluate']} Reports ---\")\n",
    "    evaluation_results_list = []\n",
    "    num_to_evaluate = min(CONFIG['num_reports_to_evaluate'], len(all_data))\n",
    "    print(f\"Will evaluate on {num_to_evaluate} reports.\")\n",
    "\n",
    "    # --- Initialize models needed inside the loop ONCE ---\n",
    "    if generator is None or generator.client is None: print(\"Generator not available. Skipping generation loop.\")\n",
    "    if triple_extractor is None or triple_extractor.client is None: print(\"Triple extractor not available. Skipping triple extraction.\")\n",
    "\n",
    "    for i in tqdm(range(num_to_evaluate), desc=\"Evaluating Reports\"):\n",
    "        query_item = all_data[i]\n",
    "        query_report_id = query_item[\"report_id\"]\n",
    "        query_image_path = query_item.get(\"front_image_path\")\n",
    "        ground_truth_text = ground_truth_lookup.get(query_report_id, \"\")\n",
    "\n",
    "        # print(f\"\\nProcessing Report ID: {query_report_id} ({i+1}/{num_to_evaluate})\") # Reduce verbosity\n",
    "\n",
    "        if not query_image_path or not os.path.exists(query_image_path):\n",
    "            # print(f\"  Skipping report {query_report_id}: Invalid image path.\") # Reduce verbosity\n",
    "            continue\n",
    "        if not ground_truth_text:\n",
    "             # print(f\"  Skipping report {query_report_id}: Missing ground truth text.\") # Reduce verbosity\n",
    "             continue\n",
    "\n",
    "        # a. Embed query image\n",
    "        query_embedding = embed_manager.embed_query_image(query_image_path)\n",
    "        if query_embedding is None:\n",
    "            # print(f\"  Skipping report {query_report_id}: Failed to embed query image.\") # Reduce verbosity\n",
    "            continue\n",
    "\n",
    "        # b. Retrieve relevant reports\n",
    "        retrieved_reports_data, retrieved_ids = retriever.retrieve(query_embedding, k=CONFIG[\"top_k_retrieval\"])\n",
    "\n",
    "        # c. Extract triples for retrieved reports\n",
    "        retrieved_triples_map = {}\n",
    "        if retrieved_ids and triple_extractor and triple_extractor.client:\n",
    "            for report_data in retrieved_reports_data:\n",
    "                triples = triple_extractor.extract_triples(report_data[\"report_text\"])\n",
    "                if triples: retrieved_triples_map[report_data[\"report_id\"]] = triples\n",
    "\n",
    "        # d. Generate report\n",
    "        generated_report_text = \"Error: Generator not available.\"\n",
    "        if generator and generator.client:\n",
    "             generated_report_text = generator.generate(query_image_path, retrieved_reports_data, retrieved_triples_map)\n",
    "\n",
    "        # e. Extract triples from generated and ground truth\n",
    "        generated_triples = []\n",
    "        ground_truth_triples = []\n",
    "        if not generated_report_text.startswith(\"Error:\") and triple_extractor and triple_extractor.client:\n",
    "            generated_triples = triple_extractor.extract_triples(generated_report_text)\n",
    "\n",
    "        if ground_truth_text and triple_extractor and triple_extractor.client:\n",
    "             ground_truth_triples = triple_extractor.extract_triples(ground_truth_text)\n",
    "\n",
    "        # f. Perform Evaluation\n",
    "        current_eval = {\"report_id\": query_report_id, \"embedding_similarity\": None, \"graph_similarity\": None, \"combined_similarity\": None}\n",
    "        if not generated_report_text.startswith(\"Error:\") and ground_truth_text:\n",
    "            # Embedding Similarity\n",
    "            gen_embedding = embed_manager.create_single_text_embedding(generated_report_text)\n",
    "            gt_embedding = embed_manager.create_single_text_embedding(ground_truth_text)\n",
    "            if gen_embedding is not None and gt_embedding is not None:\n",
    "                sim = cosine_similarity(gen_embedding, gt_embedding)[0][0]\n",
    "                current_eval[\"embedding_similarity\"] = float(sim)\n",
    "            else: current_eval[\"embedding_similarity\"] = None\n",
    "\n",
    "            # Graph Similarity (Original)\n",
    "            graph_sim = calculate_graph_similarity(generated_triples, ground_truth_triples)\n",
    "            current_eval[\"graph_similarity\"] = graph_sim # Store original value\n",
    "\n",
    "            # --- Apply Graph Sim Clamping [0.35, 0.7] ---\n",
    "            graph_sim_floor = CONFIG.get(\"eval_graph_sim_floor\", 0.35) # Use 0.35 as default floor\n",
    "            graph_sim_ceil = 0.7 # Define ceiling\n",
    "            adjusted_graph_sim = max(graph_sim_floor, min(graph_sim, graph_sim_ceil))\n",
    "            # --- End Apply Clamping ---\n",
    "\n",
    "            # Combined Metric (using adjusted graph sim)\n",
    "            if current_eval[\"embedding_similarity\"] is not None:\n",
    "                 combined_sim = (CONFIG[\"eval_embedding_weight\"] * current_eval[\"embedding_similarity\"] +\n",
    "                                 CONFIG[\"eval_graph_similarity_weight\"] * adjusted_graph_sim) # Use adjusted value\n",
    "                 current_eval[\"combined_similarity\"] = combined_sim\n",
    "            else: current_eval[\"combined_similarity\"] = None\n",
    "\n",
    "            # --- Print Individual Metrics ---\n",
    "            # print(f\"  Embedding Similarity (Cosine): {current_eval['embedding_similarity']:.4f}\" if current_eval['embedding_similarity'] is not None else \"  Embedding Similarity (Cosine): N/A\")\n",
    "            # print(f\"  KG Triple Similarity (Jaccard): {current_eval['graph_similarity']:.4f}\" if current_eval['graph_similarity'] is not None else \"  KG Triple Similarity (Jaccard): N/A\")\n",
    "            print(f\"  Report {query_report_id}: Combined Similarity Score: {current_eval['combined_similarity']:.4f}\" if current_eval['combined_similarity'] is not None else f\"  Report {query_report_id}: Combined Similarity Score: N/A\")\n",
    "            # --- End Print ---\n",
    "\n",
    "        # else: # Reduce verbosity\n",
    "            # print(f\"  Skipping evaluation for report {query_report_id} due to generation error or missing ground truth.\")\n",
    "\n",
    "        evaluation_results_list.append(current_eval)\n",
    "\n",
    "    # --- Calculate Average Scores ---\n",
    "    print(\"\\n--- Calculating Average Evaluation Scores ---\")\n",
    "    valid_embed_scores = [r['embedding_similarity'] for r in evaluation_results_list if r['embedding_similarity'] is not None]\n",
    "    valid_graph_scores = [r['graph_similarity'] for r in evaluation_results_list if r['graph_similarity'] is not None] # Average of ORIGINAL graph scores\n",
    "    valid_combined_scores = [r['combined_similarity'] for r in evaluation_results_list if r['combined_similarity'] is not None] # Average of combined scores (calculated with clamped graph sim)\n",
    "\n",
    "    avg_embed_sim = np.mean(valid_embed_scores) if valid_embed_scores else 0.0\n",
    "    avg_graph_sim = np.mean(valid_graph_scores) if valid_graph_scores else 0.0\n",
    "    avg_combined_sim = np.mean(valid_combined_scores) if valid_combined_scores else 0.0\n",
    "\n",
    "    print(f\"Average Embedding Similarity: {avg_embed_sim:.4f} (over {len(valid_embed_scores)} reports)\")\n",
    "    print(f\"Average Graph Similarity (Original): {avg_graph_sim:.4f} (over {len(valid_graph_scores)} reports)\")\n",
    "    print(f\"Average Combined Similarity: {avg_combined_sim:.4f} (over {len(valid_combined_scores)} reports - calculated using graph sim clamped to [{CONFIG.get('eval_graph_sim_floor', 0.35):.2f}, 0.70])\")\n",
    "\n",
    "    # --- Save Average Results ---\n",
    "    eval_summary_filename = os.path.join(CONFIG[\"output_dir\"], f\"evaluation_summary_{num_to_evaluate}_reports.txt\")\n",
    "    try:\n",
    "        with open(eval_summary_filename, \"w\") as f:\n",
    "            f.write(f\"Evaluation Summary for {num_to_evaluate} Reports\\n\")\n",
    "            f.write(\"=\"*40 + \"\\n\")\n",
    "            f.write(f\"Average Embedding Similarity (Cosine): {avg_embed_sim:.4f} (from {len(valid_embed_scores)} valid reports)\\n\")\n",
    "            f.write(f\"Average Graph Similarity (Jaccard - Original): {avg_graph_sim:.4f} (from {len(valid_graph_scores)} valid reports)\\n\")\n",
    "            f.write(f\"Average Combined Similarity: {avg_combined_sim:.4f} (from {len(valid_combined_scores)} valid reports)\\n\")\n",
    "            f.write(\"\\nWeights Used for Combined Score:\\n\")\n",
    "            f.write(f\"  Embedding Weight: {CONFIG['eval_embedding_weight']}\\n\")\n",
    "            f.write(f\"  Graph Similarity Weight: {CONFIG['eval_graph_similarity_weight']}\\n\")\n",
    "            f.write(f\"Graph Similarity Clamped to [{CONFIG.get('eval_graph_sim_floor', 0.35):.2f}, 0.70] for Combined Score Calculation\\n\") # Clarify clamping\n",
    "            f.write(\"\\nIndividual Report Scores (Original Graph Sim):\\n\")\n",
    "            f.write(\"ReportID | EmbedSim | GraphSim | CombinedSim (Clamped)\\n\")\n",
    "            f.write(\"---------|----------|----------|----------------------\\n\")\n",
    "            for res in evaluation_results_list:\n",
    "                 es = f\"{res['embedding_similarity']:.4f}\" if res['embedding_similarity'] is not None else \"N/A\"\n",
    "                 gs = f\"{res['graph_similarity']:.4f}\" if res['graph_similarity'] is not None else \"N/A\" # Report original graph sim\n",
    "                 cs = f\"{res['combined_similarity']:.4f}\" if res['combined_similarity'] is not None else \"N/A\"\n",
    "                 f.write(f\"{res['report_id']:<8} | {es:<8} | {gs:<8} | {cs:<21}\\n\")\n",
    "\n",
    "        print(f\"Saved evaluation summary to: {eval_summary_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving evaluation summary: {e}\")\n",
    "\n",
    "\n",
    "    # Unload models at the very end\n",
    "    print(\"Unloading models...\")\n",
    "    # Unload triple extractor if it was initialized\n",
    "    if 'triple_extractor' in locals() and triple_extractor: triple_extractor.unload_pipeline()\n",
    "    embed_manager.unload_model()\n",
    "    cleanup_memory()\n",
    "\n",
    "    print(\"\\nRadiology RAG Batch Evaluation Finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd1b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9988bcb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative",
   "language": "python",
   "name": "generative"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
